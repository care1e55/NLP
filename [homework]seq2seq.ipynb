{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgH3CAcubM4z"
   },
   "source": [
    "# Hausaufgabe\n",
    "translate(Hausaufgabe) -> **Homework**\n",
    "\n",
    "Halo!\n",
    "На семинаре мы создали простую seq2seq модель на основе rnn для перевода, а сейчас постараемся засунуть туда attention. Работать будем с тем же датасетом DE->EN (датасеты получше просто не влезают в память колаба, но если у вас есть CPU+тонна времени или GPU побольше, то можно попробовать построить перевод на WMT14 или IWSLT )\n",
    "\n",
    "В конце домашней работы предполагается написание отчета о проделанной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPuwHEnVIzn"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k #WMT14, IWSLT\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQSnhb84VLU7"
   },
   "outputs": [],
   "source": [
    "seed = 43\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L10vdpVaVXBo"
   },
   "outputs": [],
   "source": [
    "# ! python -m spacy download en\n",
    "# ! python -m spacy download de\n",
    "\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ferOqkOUVirW"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# немецкий язык является полем SRC, а английский в поле TRG\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6pNY6cWW3j5"
   },
   "outputs": [],
   "source": [
    "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOS3e7QZbLro"
   },
   "source": [
    "Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0Xpf4IBW4Uf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train : 29000\n",
      "Number of sentences in validation : 1014\n",
      "Number of sentences in test : 1000\n"
     ]
    }
   ],
   "source": [
    "labels = ['train', 'validation', 'test']\n",
    "dataloaders = [train_data, valid_data, test_data]\n",
    "for d, l in zip(dataloaders, labels):\n",
    "    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gg63m8haW4XC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in source vocabulary 7855\n",
      "Number of words in source vocabulary 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(\"Number of words in source vocabulary\", len(SRC.vocab))\n",
    "print(\"Number of words in source vocabulary\", len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSd3la5FbJ5_"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Энкодер будет ровно как в семинаре, с кдинственным изменением -- forward будет возвращать не только hidden, cell, но еще и outputs. Это нужно (надеюсь, вы уже поняли) для использования attention в декодере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ar5SN6tW4ck"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        :param: input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
    "        :param: emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
    "        :param: hid_dim is the dimensionality of the hidden and cell states.\n",
    "        :param: n_layers is the number of layers in the RNN.\n",
    "        :param: percentage of the dropout to use\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param: src sentences (src_len x batch_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8QOCpKxfD3M"
   },
   "source": [
    "## Decoder\n",
    "Оп ля, а тут уже что-то новенькое\n",
    "\n",
    "Мы будем реализовывать attention, который будет смотреть из tgt в src (НЕ self-attention). \n",
    "\n",
    "Определим два класса -- Attention и DecoderAttn. Мы разбили их на два класса, чтобы можно было играться с типом внимания, не меняя код DecoderAttn. Как вы помните с лекции, в качестве аттеншена можно брать любую странную функцию (конкатенация, маленькая сеточка, ...), и все будет работать! Поэтому вам предлагается попробовать несколько разных.\n",
    "\n",
    "\n",
    "---------------------\n",
    "Есть два подхода к реализации аттеншена:\n",
    "\n",
    "Подход #1:\n",
    "\n",
    "1. Вычисляется embed\n",
    "2. На основе hidden c прошлого шага, embedded и (возможно) enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!). Размерность batch_size * max_len, max_len -- максимальная длина предложения в батче, т.е. shape[0] от выхода энкодера.\n",
    "3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n",
    "4. Берутся attention и embedded и сворачиваются в один вектор размерности такой, чтобы кормить его self.lstm. Например, это можно сделать с помощью обычного линейного слоя\n",
    "5. Вычисляется новое скрытое состояние new_hidden. Это наша self.lstm, примененная к выходу пункта 4.\n",
    "6. Вычисляется prediction, как в семинаре\n",
    "\n",
    "Грубо говоря, вся разница с семинаром в том, что мы вместо того, чтобы embedded пихать в self.lstm, миксуем аттэншен на основе всего, что имеем (enc_out, hidden, embedded) и запихиваем в self.lstm микс аттэншена и embedded.\n",
    "\n",
    "![alt text](https://i.imgur.com/cmkRY0r.png)\n",
    "\n",
    "\n",
    "Подход #2:\n",
    "\n",
    "1. Вычисляется embed\n",
    "2. Вычисляется output, new_hidden (строчка output, (hidden, cell) = self.rnn(embedded, (hidden, cell)))\n",
    "3. На основе output и enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!)\n",
    "3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n",
    "4. Вычисляется prediction на основе attention и output. Можно, например, взять nn.Linear() от конкатенации attention и output.\n",
    "\n",
    "Разница с первым подходом в том, что мы сначала вычисляем выход rnn слоя, а потом смотрим вниманием на src и на основе выхода rnn и attn считаем выход (prediction). \n",
    "\n",
    "![alt text](https://i.imgur.com/5aWjQWv.png)\n",
    "\n",
    "\n",
    "Вам предлагается реализовать хотя бы 1 из вариантов и хотя бы 2 варианта функции attention (в классе Attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRgtzaf4bJp6"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim, method=\"one-layer-net\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if method == \"one-layer-net\":\n",
    "            self.fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        elif method == \"concat\":\n",
    "            self.fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(hidden_dim,1))\n",
    "        \n",
    "    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n",
    "        if self.method == \"one-layer-net\":\n",
    "            out = self.fc(last_hidden)\n",
    "            return torch.bmm(encoder_outputs.permute(1, 0, 2), out.permute(1, 2, 0)).squeeze(-1).permute(1,0)\n",
    "        elif self.method == \"concat\":\n",
    "            out = torch.tanh(self.fc(last_hidden+encoder_outputs))\n",
    "            out = out.permute(1, 0, 2)\n",
    "            return out.matmul(self.weight).squeeze(-1).permute(1,0)\n",
    "\n",
    "\n",
    "class DecoderAttn(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, attention, dropout=0.1):\n",
    "        super(DecoderAttn, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.attn = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers) #(lstm embd, hid, layers, dropout)\n",
    "        self.out = nn.Linear(self.hid_dim*2, self.output_dim) # Projection :hid_dim x output_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_output):\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        alignment_scores = self.attn(output, encoder_output)\n",
    "        attn_weights = F.softmax(alignment_scores, dim = 0)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0).permute(2,0,1), encoder_output.permute(1,0,2)).permute(1,0,2)\n",
    "        output = torch.cat((output, context_vector),-1)\n",
    "        prediction = self.out(output[0])\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlD7-nusfL86"
   },
   "source": [
    "## Seq2Seq module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOineJlHpof2"
   },
   "source": [
    "Здесь опять ничего не поменяется кроме того, что энкодер теперь возвращает свой output, а декодер его принимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_YvVGzaW4fY"
   },
   "outputs": [],
   "source": [
    "BOS_IDX = SRC.vocab.stoi['<sos>']\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self._init_weights() \n",
    "        self.max_len=30\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        :param: src (src_len x batch_size)\n",
    "        :param: tgt\n",
    "        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_out, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, enc_out)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def translate(self, src):\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = []\n",
    "        src = torch.tensor(src).to(self.device)\n",
    "        sent_vec = SRC.process([src]).to(device)\n",
    "        sent_vec = src.permute(1,0)\n",
    "        enc_out, hidden, cell = self.encoder(sent_vec)\n",
    "        \n",
    "        input = torch.tensor([BOS_IDX]).to(self.device)\n",
    "\n",
    "        for t in range(1, self.max_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, enc_out)\n",
    "            top1 = output.max(1)[1]\n",
    "            outputs.append(top1)\n",
    "            input = (top1)\n",
    "        return outputs\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        p = 0.08\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param.data, -p, p)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msbn2VypfUur"
   },
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "src_embd_dim =  tgt_embd_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers =  2\n",
    "dropout_prob = 0.2\n",
    "\n",
    "batch_size = 64\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                  batch_size = batch_size, device = device)\n",
    "train_iterator, valid_iterator, test_iterator = iterators\n",
    "\n",
    "atn = Attention(batch_size, hidden_dim, method=\"concat\")\n",
    "enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
    "dec = DecoderAttn(output_dim, tgt_embd_dim, hidden_dim, num_layers, atn, dropout_prob)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5V9ZnK4fUxq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderAttn(\n",
       "    (attn): Attention(\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2)\n",
       "    (out): Linear(in_features=1024, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vaUeDjTfU4k"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfbTx2FMjaIM"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing !!\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " Train Loss 4.5697  Val loss 4.28349:\n",
      "Train Perplexity 96.51515088396333  Val Perplexity 72.49299960309547:\n",
      "Epoch: 1 \n",
      " Train Loss 3.35817  Val loss 3.64849:\n",
      "Train Perplexity 28.736554836602078  Val Perplexity 38.41661314427171:\n",
      "Epoch: 2 \n",
      " Train Loss 2.73932  Val loss 3.39174:\n",
      "Train Perplexity 15.476457526253919  Val Perplexity 29.717615961695998:\n",
      "Epoch: 3 \n",
      " Train Loss 2.34848  Val loss 3.31621:\n",
      "Train Perplexity 10.469643765541248  Val Perplexity 27.555716237187013:\n",
      "Epoch: 4 \n",
      " Train Loss 2.06601  Val loss 3.3209:\n",
      "Train Perplexity 7.893266071782104  Val Perplexity 27.68525607982288:\n",
      "Epoch: 5 \n",
      " Train Loss 1.84076  Val loss 3.38189:\n",
      "Train Perplexity 6.301325449006046  Val Perplexity 29.426334361423248:\n",
      "Epoch: 6 \n",
      " Train Loss 1.63626  Val loss 3.28277:\n",
      "Train Perplexity 5.135925187577853  Val Perplexity 26.649489641080965:\n",
      "Epoch: 7 \n",
      " Train Loss 1.46783  Val loss 3.39328:\n",
      "Train Perplexity 4.3398075334603545  Val Perplexity 29.763416347522412:\n",
      "Epoch: 8 \n",
      " Train Loss 1.34174  Val loss 3.39783:\n",
      "Train Perplexity 3.825694425688326  Val Perplexity 29.89914844826676:\n",
      "Epoch: 9 \n",
      " Train Loss 1.18892  Val loss 3.44852:\n",
      "Train Perplexity 3.2835330760149617  Val Perplexity 31.45380621034731:\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "CLIP = 1\n",
    "\n",
    "# TODO\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
    "    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
    "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "src_embd_dim =  tgt_embd_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers =  2\n",
    "dropout_prob = 0.2\n",
    "\n",
    "batch_size = 64\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                  batch_size = batch_size, device = device)\n",
    "train_iterator, valid_iterator, test_iterator = iterators\n",
    "\n",
    "atn = Attention(batch_size, hidden_dim, method=\"one-layer-net\")\n",
    "enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
    "dec = DecoderAttn(output_dim, tgt_embd_dim, hidden_dim, num_layers, atn, dropout_prob)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQV_yqkLjcyQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " Train Loss 4.84992  Val loss 4.64837:\n",
      "Train Perplexity 127.73017102359952  Val Perplexity 104.41465091108981:\n",
      "Epoch: 1 \n",
      " Train Loss 3.81275  Val loss 4.1137:\n",
      "Train Perplexity 45.27477345490371  Val Perplexity 61.17263811991019:\n",
      "Epoch: 2 \n",
      " Train Loss 3.27865  Val loss 3.86091:\n",
      "Train Perplexity 26.539919613007946  Val Perplexity 47.5085644975907:\n",
      "Epoch: 3 \n",
      " Train Loss 2.96432  Val loss 3.7336:\n",
      "Train Perplexity 19.38151932630632  Val Perplexity 41.829423358429345:\n",
      "Epoch: 4 \n",
      " Train Loss 2.72551  Val loss 3.66943:\n",
      "Train Perplexity 15.264196682395824  Val Perplexity 39.229538649224885:\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "CLIP = 1\n",
    "\n",
    "# TODO\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 5e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
    "    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
    "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5Zf6Kb1jhOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.490470603108406 Test PPL:32.80138050621148|\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_IDX = SRC.vocab.stoi['<eos>']\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    function that uses .translate() method of the model to translate german sentence into english\n",
    "    params:\n",
    "        sentence: tokenized gernam sentence\n",
    "    \"\"\"\n",
    "    out = []\n",
    "#     sentence = sentence.lower()\n",
    "    sent_vec = [BOS_IDX] + [SRC.vocab.stoi[token] for token in sentence] + [EOS_IDX]\n",
    "    sent_vec += [PAD_IDX]*(30-len(sent_vec))\n",
    "    translation_idx = model.translate(torch.tensor([sent_vec]))\n",
    "    for t in translation_idx:\n",
    "        if t[0] != EOS_IDX:\n",
    "#             print(TRG.vocab.itos[t[0]], end=' ')\n",
    "            out.append(TRG.vocab.itos[t[0]])\n",
    "        else:\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/progz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['an', 'adult', 'of', 'young', 'children', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"ein klein apfel\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sP2TiDm18gyi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/progz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'with',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'in',\n",
       " 'a',\n",
       " 'red',\n",
       " 'and',\n",
       " 'red',\n",
       " 'dress',\n",
       " 'and',\n",
       " 'black',\n",
       " 'boots',\n",
       " 'on',\n",
       " 'the',\n",
       " 'street',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"stiefeln schwarzen und kleid rot-weißen einem in frau einer mit straße der auf fotoshooting ein\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBrHtQ_qrSKX"
   },
   "source": [
    "ИИИИ давайте также научимся считать самую популярную метрику для перевода -- BLEU (https://en.wikipedia.org/wiki/BLEU)\n",
    "\n",
    "В общем-то, вам повезло -- ее писать руками скучно, да и nltk ее написало за вас:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFIVyXXeJrCr"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def compute_bleu(inp_lines, out_lines):\n",
    "    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
    "    translations = [translate(line) for line in inp_lines]\n",
    "    return corpus_bleu([[ref] for ref in out_lines], translations) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_bleu([i.src for i in train_data], [j.trg for j in train_data]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/progz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.96331779698981"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu([i.src for i in valid_data], [j.trg for j in valid_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/progz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.067326562529466"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu([i.src for i in test_data], [j.trg for j in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kuf44w8R9WnV"
   },
   "source": [
    "Если вы реализовали несколько методов аттеншена, опишите каждый из них и полученную метрику на нем в отчете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tk5L17Th9WnV"
   },
   "source": [
    "В реализации использовались материалы семинара и статья https://blog.floydhub.com/attention-mechanism/#luong-att-step2\n",
    "Реализован способ 2 и 2 вида подсчета score attention - concat и one-layer (general)\n",
    "В one-layer hidden декодера передается в линейный слой - формируется вектор весов размерности hidden который будут весам для encoder output когда мы их меремножим. Самом перемножение делается по всему батчу для чего приходилось сделать permute чтобы размерности батчей были по нулевой оси в соответвии со спецификацией метода bmm. Берем от этого softmax и получили attention по всему батчу. Далее это также тензорно перемножается (опять по батчу с permute) с выходом энкодера получая новый взвешенный вектор контекста. Output с контекстом конкатенируются и через линейный слой размерности словаря применяется softmax для предсказания наиболее верятного слова.\n",
    "В concat вектора decoder_hidden и encoder_output складываются и применяется линейный слой размерности hidden и с активацией tanh - выход матрично (побатчево) перемножается с весами. Остальное аналогично.\n",
    "Переводы слов из тренировочной выборки переводятся примерно похоже, в отличии от слов которых модель могла не видеть. В методе транслате в качестве input передается символ начала предложения как стартовый и далее слова предсказываются по контектсу с attention.\n",
    "В ходе обучения для каждого из методов получились примерно одинаковые PPL ~30, loss ~3.5 и BLEU ~ 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]seq2seq_and_attn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
