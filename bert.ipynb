{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO_wCxS951bT"
   },
   "source": [
    "# Homework\n",
    "\n",
    "Привет! В этой домашнем задании ты научишься обучении модели BERT. На семинаре был разобран код модели, здесь же посмотрим на то, как надо обработать данные, чтобы на них модель могла учиться. \n",
    "\n",
    "Замечания по выполнению задания:\n",
    "\n",
    "- Код внутри блока `<DON'T TOUCH THIS!>` используется для проверки задания, его нельзя трогать. \n",
    "\n",
    "- Внутри блока `<YOUR CODE>` может больше кода, чем там показано изначально.\n",
    "\n",
    "- От задания требуется написания небольшого отчета в конце.\n",
    "\n",
    "\n",
    "Для начала загрузи нужные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dq4NkRu_u3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting catalyst\n",
      "  Downloading https://files.pythonhosted.org/packages/64/05/ac1e89cd9533c66e10e286d23dc2c4d042abee177bba9d1be738f55fa10d/catalyst-20.5-py2.py3-none-any.whl (347kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers)\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
      "Collecting tokenizers==0.7.0 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.8MB 443kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting requests (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/cd/cc0c981b9d69345bad895c80bf7e48e43fcef4dc07539a68e4e9160b135c/regex-2020.5.7-cp36-cp36m-manylinux1_x86_64.whl (675kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting deprecation (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
      "Collecting pandas>=0.22 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/71/8f53bdbcbc67c912b888b40def255767e475402e9df64050019149b1a943/pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting torch>=1.1.0 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting crc32c>=1.7 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/82/f60248c01a8a23ae07bd4c43d78d69b20ffe324311db3b0785e391aa09d2/crc32c-2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tensorboardX (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl\n",
      "Collecting ipython (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/00/afc3968a3cdf5f30c5c9dfb8e6a61e63231d6869a461dc1ff418280c5ea4/ipython-7.14.0-py3-none-any.whl\n",
      "Collecting PyYAML (from catalyst)\n",
      "Collecting scikit-learn>=0.20 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting GitPython>=3.1.1 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl\n",
      "Collecting tensorboard>=1.14.0 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl\n",
      "Collecting plotly>=4.1.0 (from catalyst)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/78/eb6cbe96c8379c54819592bb228c58ed7386fcc60a55eca7db99432fdf14/plotly-4.7.1-py2.py3-none-any.whl (11.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.5MB 184kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting packaging (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/0a/34641d2bf5c917c96db0ded85ae4da25b6cd922d6b794648d4e7e07c88e5/packaging-20.3-py2.py3-none-any.whl\n",
      "Collecting six (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Collecting click (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/2b/26e37a4b034800c960a00c4e1b3d9ca5d7014e983e6e729e33ea2f36426c/certifi-2020.4.5.1-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\n",
      "Collecting pytz>=2017.2 (from pandas>=0.22->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.6.1 (from pandas>=0.22->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting future (from torch>=1.1.0->catalyst)\n",
      "Collecting protobuf>=3.8.0 (from tensorboardX->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting traitlets>=4.2 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl\n",
      "Collecting jedi>=0.10 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/3f/67f027e18c60a800875df1a0894a2436ce9053637fa39725766e937c0a71/jedi-0.17.0-py2.py3-none-any.whl\n",
      "Collecting backcall (from ipython->catalyst)\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n",
      "Collecting pickleshare (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/e4/a7/81b39aa50e9284fe2cb21cc7fb7de7817b224172d42793fd57451d38842b/prompt_toolkit-3.0.5-py3-none-any.whl\n",
      "Collecting setuptools>=18.5 (from ipython->catalyst)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl\n",
      "Collecting decorator (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting pygments (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl\n",
      "Collecting scipy>=0.17.0 (from scikit-learn>=0.20->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1.1->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl\n",
      "Collecting absl-py>=0.4 (from tensorboard>=1.14.0->catalyst)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=1.14.0->catalyst)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 4.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel>=0.26; python_version >= \"3\" (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard>=1.14.0->catalyst)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/45/c562c8d2dadc3dc97347d1d7b218ef94be6d851fd6177dad96fdf5e1c74c/google_auth-1.14.2-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 15.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/cd/a0c1f9e4582ea64dddf76c1b808b318d01e3b858a51c715bffab1016ecc7/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.24.3 (from tensorboard>=1.14.0->catalyst)\n",
      "Collecting retrying>=1.3.3 (from plotly>=4.1.0->catalyst)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cycler>=0.10 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Collecting parso>=0.7.0 (from jedi>=0.10->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b5/61/998cce9e7476de000d031874df26a18f67cb73448164fc44a98f0c55920b/parso-0.7.0-py2.py3-none-any.whl\n",
      "Collecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/d5/1ecdac957e3ea12c1b319fcdee8b6917ffaff8b4644d673c4d72d2f20b49/wcwidth-0.1.9-py2.py3-none-any.whl\n",
      "Collecting smmap<4,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.1.1->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/e4/891bfcaf868ccabc619942f27940c77a8a4b45fd8367098955bb7e152fb1/importlib_metadata-1.6.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, joblib, regex, tqdm, click, sacremoses, dataclasses, tokenizers, numpy, certifi, chardet, idna, urllib3, requests, filelock, sentencepiece, transformers, pyparsing, packaging, deprecation, pytz, python-dateutil, pandas, future, torch, crc32c, setuptools, protobuf, tensorboardX, ipython-genutils, decorator, traitlets, parso, jedi, backcall, ptyprocess, pexpect, pickleshare, wcwidth, prompt-toolkit, pygments, ipython, PyYAML, scipy, scikit-learn, smmap, gitdb, GitPython, absl-py, zipp, importlib-metadata, markdown, wheel, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, werkzeug, grpcio, tensorboard, retrying, plotly, kiwisolver, cycler, matplotlib, catalyst\n",
      "Successfully installed GitPython-3.1.2 PyYAML-5.3.1 absl-py-0.9.0 backcall-0.1.0 cachetools-4.1.0 catalyst-20.5 certifi-2020.4.5.1 chardet-3.0.4 click-7.1.2 crc32c-2.0 cycler-0.10.0 dataclasses-0.7 decorator-4.4.2 deprecation-2.1.0 filelock-3.0.12 future-0.18.2 gitdb-4.0.5 google-auth-1.14.2 google-auth-oauthlib-0.4.1 grpcio-1.28.1 idna-2.9 importlib-metadata-1.6.0 ipython-7.14.0 ipython-genutils-0.2.0 jedi-0.17.0 joblib-0.14.1 kiwisolver-1.2.0 markdown-3.2.2 matplotlib-3.2.1 numpy-1.18.4 oauthlib-3.1.0 packaging-20.3 pandas-1.0.3 parso-0.7.0 pexpect-4.8.0 pickleshare-0.7.5 plotly-4.7.1 prompt-toolkit-3.0.5 protobuf-3.11.3 ptyprocess-0.6.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.6.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 regex-2020.5.7 requests-2.23.0 requests-oauthlib-1.3.0 retrying-1.3.3 rsa-4.0 sacremoses-0.0.43 scikit-learn-0.22.2.post1 scipy-1.4.1 sentencepiece-0.1.86 setuptools-46.1.3 six-1.14.0 smmap-3.0.4 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorboardX-2.0 tokenizers-0.7.0 torch-1.5.0 tqdm-4.46.0 traitlets-4.3.3 transformers-2.9.0 urllib3-1.25.9 wcwidth-0.1.9 werkzeug-1.0.1 wheel-0.34.2 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlHnoGZN6OKy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AccuracyCallback, SchedulerCallback, F1ScoreCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmlqBOSO7a5L"
   },
   "source": [
    "Внизу идет технический код, который нужен для загрузки датасетов. Его можно уменьшить, выбрав только некоторые из них. Для того, что бы зачесть задание, надо выбрать не менее двух задач, для хотя бы одной из которых нужно использовать два предложения(ответ и вопрос, два предложения и прочее). Подробнее про датасеты [здесь](https://gluebenchmark.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4H6_6zgnfRQR"
   },
   "outputs": [],
   "source": [
    "# TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\"]\n",
    "# TASK2PATH = {\n",
    "#     \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",\n",
    "#     \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",\n",
    "#     \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",\n",
    "#     \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\",\n",
    "#     \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",\n",
    "#     \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",\n",
    "#     \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",\n",
    "#     \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",\n",
    "#     \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",\n",
    "#     \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",\n",
    "# }\n",
    "\n",
    "# MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
    "# MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
    "\n",
    "# data_dir = \"data/\"\n",
    "# max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\"SST\", \"QQP\"]\n",
    "TASK2PATH = {\n",
    "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",\n",
    "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\"\n",
    "}\n",
    "\n",
    "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
    "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
    "\n",
    "data_dir = \"data/\"\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4O8Y-go7czun"
   },
   "outputs": [],
   "source": [
    "def download_and_extract(task, data_dir):\n",
    "    print(\"Downloading and extracting %s...\" % task)\n",
    "    data_file = \"%s.zip\" % task\n",
    "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
    "    with zipfile.ZipFile(data_file) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_file)\n",
    "    print(\"\\tCompleted!\")\n",
    "\n",
    "def format_mrpc(data_dir, path_to_data):\n",
    "    print(\"Processing MRPC...\")\n",
    "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
    "    if not os.path.isdir(mrpc_dir):\n",
    "        os.mkdir(mrpc_dir)\n",
    "    if path_to_data:\n",
    "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
    "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
    "    else:\n",
    "        print(\"Local MRPC data not specified, downloading data from %s\" % MRPC_TRAIN)\n",
    "        mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
    "        mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
    "        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
    "        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
    "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
    "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
    "    urllib.request.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
    "\n",
    "    dev_ids = []\n",
    "    with open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding=\"utf8\") as ids_fh:\n",
    "        for row in ids_fh:\n",
    "            dev_ids.append(row.strip().split(\"\\t\"))\n",
    "\n",
    "    with open(mrpc_train_file, encoding=\"utf8\") as data_fh, open(\n",
    "        os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\"\n",
    "    ) as train_fh, open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh:\n",
    "        header = data_fh.readline()\n",
    "        train_fh.write(header)\n",
    "        dev_fh.write(header)\n",
    "        for row in data_fh:\n",
    "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
    "            if [id1, id2] in dev_ids:\n",
    "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "            else:\n",
    "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "\n",
    "    with open(mrpc_test_file, encoding=\"utf8\") as data_fh, open(\n",
    "        os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\"\n",
    "    ) as test_fh:\n",
    "        header = data_fh.readline()\n",
    "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "        for idx, row in enumerate(data_fh):\n",
    "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
    "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
    "    print(\"\\tCompleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4hNWuZ5okuj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting SST...\n",
      "\tCompleted!\n",
      "Downloading and extracting QQP...\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "TASKS = [\"SST\", \"QQP\"] # Или можно просто сюда вписать те датасеты, которые ты выбрал.\n",
    "\n",
    "for task in TASKS:\n",
    "    if task == \"MRPC\":\n",
    "        format_mrpc(data_dir, None)\n",
    "    else:\n",
    "        download_and_extract(task, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5S3iSoK8LOG"
   },
   "source": [
    "Загрузи один из выбранных датасет с помощью Pandas(не обязательно через него, но так проще) и посмотри на него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67349.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.557826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  67349.000000\n",
       "mean       0.557826\n",
       "std        0.496649\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/SST-2/train.tsv\", sep=\"\\t\")\n",
    "data.describe()\n",
    "# 67349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBn6ejxaokw1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it 's a charming and often affecting journey .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unflinchingly bleak and desperate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows us to hope that nolan is poised to emba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it 's slow -- very , very slow .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0    it 's a charming and often affecting journey .       1\n",
       "1                 unflinchingly bleak and desperate       0\n",
       "2  allows us to hope that nolan is poised to emba...      1\n",
       "3  the acting , costumes , music , cinematography...      1\n",
       "4                  it 's slow -- very , very slow .       0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd, valid_pd = train_test_split(data, test_size=0.2)\n",
    "test_pd = pd.read_csv(\"./data/SST-2/dev.tsv\", sep=\"\\t\")\n",
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iK5iar5f8e_7"
   },
   "source": [
    "Для начала рассмотрим важную часть обработки текста для трансфомера(и не только) – токенайзер.\n",
    "\n",
    "В качестве примера токенайзера воспользуемся внутренним из библиотеки transformers, обученным для BERT-а. Посмотрим, что он умеет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8I3h53-DpofT"
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0BjZAx68v6T"
   },
   "source": [
    "Посмотрим, как происходит токенизация предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDkvs3uQpsCY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units', '.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Hide new secretions from the parental units.\"\n",
    "print(tokenizer.tokenize(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La_1q8Cj82e8"
   },
   "source": [
    "Видно, что предложения разделяются не на слова, а подслова. Токены, которые надо объеденить в слова для получения \"нормального\" текста, выделены с помощью `##`. Посмотрим, как различаются коды токенов с этим символом и без него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKd0SwME9bi3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15956]\n",
      "[8496]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(['ions']))\n",
    "print(tokenizer.convert_tokens_to_ids(['##ions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5qZXD5Z-D5o"
   },
   "source": [
    "Для токенизации предложений воспользуемся методом `encode`. Она принимает предложение как строку или список токенов**(!)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUZQ6tMNpsEh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiVCb6RG-Yi2"
   },
   "source": [
    "Добавились специальные токены впереди и сзади предложения. Посмотрим на весь список специальных токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AqoVIrktk4v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "{'[UNK]': 100, '[CLS]': 101, '[MASK]': 103, '[SEP]': 102, '[PAD]': 0}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print({i: j for i, j in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TA4XbeA-phM"
   },
   "source": [
    "Посмотрим, что ещё может делать токенайзер. Что требуется нам для обучения BERT-а: добавить паддинг, получить маску аттеншена и тип токенов. Попробуем сделать это самостоятельно и посмотрим, как это сделать с помощью токенайзера.\n",
    "\n",
    "Выбери два предложения из обучающей выборки. Получи их токены с помощью метода `tokenize`. Объедени списки токенов так, чтобы модель могла различать, что они от разных предложений. \n",
    "\n",
    "(Подсказка: на семинаре была картинка с эмбеддингами. Она может подсказать, что надо изменить в токенах предложения) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hide new secretions from the parental units '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.iloc[0:2].sentence[0]\n",
    "# train_pd.iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4esxYXTipsG8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07d9d5788513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# <DON'T TOUCH THIS!>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_union\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not equal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# </DON'T TOUCH THIS!>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_union' is not defined"
     ]
    }
   ],
   "source": [
    "# <YOUR CODE>\n",
    "s1, s2 = train_pd.iloc[0].sentence, train_pd.iloc[1].sentence\n",
    "tokenized_s1, tokenized_s2 = tokenizer.encode(s1), tokenizer.encode(s2)\n",
    "tokenized_union = tokenized_s1 + tokenized_s2[1:] \n",
    "# </YOUR CODE>\n",
    "\n",
    "# <DON'T TOUCH THIS!>\n",
    "assert tokenizer.encode(s_union) == tokenizer.encode(s1, s2), \"Not equal\"\n",
    "# </DON'T TOUCH THIS!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9k6nrFRHBJBl"
   },
   "source": [
    "Теперь надо добавь нулей в полученный список чисел, чтобы они легко складывались в батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "encoded_full = tokenized_union + [0] * (max_seq_length - len(tokenized_union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY-xd0_qp9rJ"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "# <YOUR CODE>\n",
    "encoded_full = tokenized_union + [0] * (max_seq_length - len(tokenized_union))\n",
    "# </YOUR CODE>\n",
    "\n",
    "# <DON'T TOUCH THIS!>\n",
    "encoded_correct = tokenizer.encode(s1, s2, max_length=max_seq_length, pad_to_max_length=True)\n",
    "assert len(encoded_full) == len(encoded_correct), \"Different length\"\n",
    "assert encoded_full == encoded_correct, \"Not equal\"\n",
    "# </DON'T TOUCH THIS!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd5yNVukBZuC"
   },
   "source": [
    "В модель также надо кинуть маску для механизма внимания и тип предложения для каждого токена. Сделай их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hiljf3_vQ75"
   },
   "outputs": [],
   "source": [
    "# <YOUR CODE>\n",
    "token_type_ids = [int(i > encoded_full.index(102) and x != 0) for i, x in enumerate(encoded_full) ]\n",
    "attention_mask = [ int(i!=0) for i in encoded_full ]\n",
    "# </YOUR CODE>\n",
    "\n",
    "# <DON'T TOUCH THIS!>\n",
    "encoded_plus = tokenizer.encode_plus(train['sentence'][0], text_pair=train['sentence'][1], max_length=max_seq_length, pad_to_max_length=True)\n",
    "assert len(token_type_ids) == len(encoded_plus['token_type_ids']), \"Different length in token_type_ids\"\n",
    "assert token_type_ids == encoded_plus['token_type_ids'], \"Not equal token_type_ids\"\n",
    "assert len(attention_mask) == len(encoded_plus['attention_mask']), \"Different length in attention_mask\"\n",
    "assert attention_mask == encoded_plus['attention_mask'], \"Not equal attention_mask\"\n",
    "# </DON'T TOUCH THIS!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxruIU08CEgg"
   },
   "source": [
    "Как видно из тестов, все нужные для обработки текста для BERT-а вещи может делать токенизатор из `transformers`. Но не все токенизаторы настолько функциональны. Их (почти)полный список:\n",
    "- [Sentence Piece](https://github.com/google/sentencepiece/)\n",
    "- [fastBPE](https://github.com/glample/fastBPE)\n",
    "- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "- [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe)\n",
    "\n",
    "Их сравнивают [здесь](https://github.com/VKCOM/YouTokenToMe/blob/master/benchmark.md) или [здесь](https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6). Также специальные токенайзеры, которые специализируются на \"незападные\" языки. Но не будем на них останавливаться.\n",
    "\n",
    "Теперь ты знаешь достаточно, чтобы написать обработчик данных. Что надо сделать: получить из данных предложения, закодировать их, получить аттенш маску и тип токенов, не забыть про таргет. \n",
    "\n",
    "P.S. Есть более быстрая версия токенизатора для BERT внутри `transformers`, `BertTokenizerFast`. \n",
    "\n",
    "P.S.S. Теперь надо использовать только функционал токенайзера для кодирования предложений, без велосипедов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0y-wgeHHokz1"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        self.encoded_plus = [ \n",
    "            tokenizer.encode_plus(item, max_length=max_seq_length, pad_to_max_length=True) \n",
    "            for item in data['sentence']\n",
    "        ] \n",
    "        \n",
    "#         print(self.encoded_plus[0])\n",
    "        \n",
    "        self.input_ids = torch.tensor([ i['input_ids'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.attention_mask = torch.tensor([ i['attention_mask'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.token_type_ids = torch.tensor([ i['token_type_ids'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.target = torch.tensor(list(self.data['label']), dtype=torch.long) \n",
    "        # </YOUR CODE>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'token_type_ids': self.token_type_ids[idx],\n",
    "            'targets': self.target[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFVoCAkiFGJt"
   },
   "source": [
    "Воспользуйтесь семинаром и построй модель для классификации предложений.\n",
    "\n",
    "(Подсказка: весь код BERT-а из семинара доступен из библиотеки `transformers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa6coCLFFSZW"
   },
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name: str, num_labels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        \n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            pretrained_model_name,\n",
    "            num_labels = num_labels\n",
    "        )\n",
    "        \n",
    "        self.bert = transformers.BertModel.from_pretrained(\n",
    "            pretrained_model_name, \n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # </YOUR CODE>\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        bert_output = self.bert(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        hidden_state = bert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        # <YOUR CODE>\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "        # </YOUR CODE>\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gI9IMsE0G1gh"
   },
   "source": [
    "Выбери из [списка](https://huggingface.co/models?search=google%2Fbert_) несколько моделей, которые ты будешь обучать. Сравни их качество на выбранных датасетах. \n",
    "\n",
    "Лучше всего будет выбрать одну основную конфигурацию, и другие с небольшим изменением. Например, пройтись по такой сетке: `{'layers': [2, 4], 'num_heads': [2, 4]}`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем google/bert_uncased_L-4_H-256_A-4 на SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-22wBwrFMob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "# <YOUR CODE>\n",
    "pretrained_model_name = 'google/bert_uncased_L-4_H-256_A-4'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
    "# </YOUR CODE>\n",
    "\n",
    "model.to(device)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCL7wstMczw9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1684\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "# <YOUR CODE>\n",
    "train_dataset = TextClassificationDataset(train_pd, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_dataset = TextClassificationDataset(valid_pd, tokenizer)\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TextClassificationDataset(test_pd, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader,\n",
    "    \"test\": test_dataloader    \n",
    "}\n",
    "# </YOUR CODE>\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VsyoAmwjnb2"
   },
   "outputs": [],
   "source": [
    "seed = 404\n",
    "set_global_seed(seed)\n",
    "prepare_cudnn(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38ojXge_Hy71"
   },
   "outputs": [],
   "source": [
    "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "warmup_steps = len(train_dataloader) // 2\n",
    "t_total = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3TVwcWzH1ok"
   },
   "source": [
    "Добавь Loss, Optimizer и Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mifqwFYdjnWx"
   },
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# <YOUR CODE>\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "# </YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbbWyPiE7MgP"
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTXx8X69IClJ"
   },
   "source": [
    "Для обучения модели воспользуемся библиотекой `catalyst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFZ9z53VE5tB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
      "\n",
      "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 15:17:33,862] \n",
      "1/10 * Epoch 1 (_base): lr=9.474e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): accuracy01=0.7167 | loss=0.5280 | lr=7.371e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (valid): accuracy01=0.8319 | loss=0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
      "\n",
      "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 15:20:38,030] \n",
      "2/10 * Epoch 2 (_base): lr=8.421e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): accuracy01=0.8489 | loss=0.3470 | lr=8.947e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (valid): accuracy01=0.8704 | loss=0.3078\n",
      "[2020-05-11 15:23:46,565] \n",
      "3/10 * Epoch 3 (_base): lr=7.368e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): accuracy01=0.8744 | loss=0.2965 | lr=7.894e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (valid): accuracy01=0.8841 | loss=0.2806\n",
      "[2020-05-11 15:26:50,754] \n",
      "4/10 * Epoch 4 (_base): lr=6.316e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): accuracy01=0.8925 | loss=0.2598 | lr=6.842e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (valid): accuracy01=0.8944 | loss=0.2587\n",
      "[2020-05-11 15:29:54,739] \n",
      "5/10 * Epoch 5 (_base): lr=5.263e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): accuracy01=0.9040 | loss=0.2360 | lr=5.789e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (valid): accuracy01=0.9039 | loss=0.2470\n",
      "[2020-05-11 15:32:58,738] \n",
      "6/10 * Epoch 6 (_base): lr=4.211e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): accuracy01=0.9145 | loss=0.2151 | lr=4.737e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (valid): accuracy01=0.9085 | loss=0.2364\n",
      "[2020-05-11 15:36:02,785] \n",
      "7/10 * Epoch 7 (_base): lr=3.158e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): accuracy01=0.9191 | loss=0.2038 | lr=3.684e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (valid): accuracy01=0.9113 | loss=0.2290\n",
      "[2020-05-11 15:39:06,527] \n",
      "8/10 * Epoch 8 (_base): lr=2.105e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): accuracy01=0.9242 | loss=0.1923 | lr=2.631e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (valid): accuracy01=0.9128 | loss=0.2288\n",
      "[2020-05-11 15:42:10,282] \n",
      "9/10 * Epoch 9 (_base): lr=1.053e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): accuracy01=0.9254 | loss=0.1888 | lr=1.579e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (valid): accuracy01=0.9156 | loss=0.2241\n",
      "[2020-05-11 15:45:14,028] \n",
      "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): accuracy01=0.9291 | loss=0.1825 | lr=5.260e-07 | momentum=0.9000\n",
      "10/10 * Epoch 10 (valid): accuracy01=0.9145 | loss=0.2261\n",
      "Top best models:\n",
      "logs/checkpoints/train.9.pth\t0.2241\n"
     ]
    }
   ],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader\n",
    "}\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"token_type_ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_labels),\n",
    "        SchedulerCallback(mode='batch'),\n",
    "    ],\n",
    "    logdir=log_dir,\n",
    "    num_epochs=epochs,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555045871559633"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = runner.predict_loader(model=model, loader=test_dataloader, resume=log_dir+'checkpoints/best.pth')\n",
    "y_pred = []\n",
    "counter = 0\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for i in logits:\n",
    "    for j in softmax(i['logits']).argmax(axis=1).tolist():\n",
    "        y_pred.append(j)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "for i in test_dataloader:\n",
    "    for j in i['targets'].tolist():\n",
    "        y_true.append(j)\n",
    "        \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем google/bert_uncased_L-2_H-128_A-2 на SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "# <YOUR CODE>\n",
    "pretrained_model_name = 'google/bert_uncased_L-2_H-128_A-2'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
    "# </YOUR CODE>\n",
    "\n",
    "model.to(device)\n",
    "print(\"Success!\")\n",
    "\n",
    "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "warmup_steps = len(train_dataloader) // 2\n",
    "t_total = len(train_dataloader) * epochs\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# <YOUR CODE>\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "# </YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
      "\n",
      "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 15:48:27,478] \n",
      "1/10 * Epoch 1 (_base): lr=9.474e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): accuracy01=0.6029 | loss=0.6739 | lr=7.371e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (valid): accuracy01=0.7538 | loss=0.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
      "\n",
      "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 15:49:17,749] \n",
      "2/10 * Epoch 2 (_base): lr=8.421e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): accuracy01=0.7770 | loss=0.4663 | lr=8.947e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (valid): accuracy01=0.8232 | loss=0.3970\n",
      "[2020-05-11 15:50:07,414] \n",
      "3/10 * Epoch 3 (_base): lr=7.368e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): accuracy01=0.8188 | loss=0.4004 | lr=7.894e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (valid): accuracy01=0.8428 | loss=0.3633\n",
      "[2020-05-11 15:50:57,017] \n",
      "4/10 * Epoch 4 (_base): lr=6.316e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): accuracy01=0.8374 | loss=0.3670 | lr=6.842e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (valid): accuracy01=0.8547 | loss=0.3405\n",
      "[2020-05-11 15:51:46,601] \n",
      "5/10 * Epoch 5 (_base): lr=5.263e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): accuracy01=0.8496 | loss=0.3434 | lr=5.789e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (valid): accuracy01=0.8621 | loss=0.3277\n",
      "[2020-05-11 15:52:36,146] \n",
      "6/10 * Epoch 6 (_base): lr=4.211e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): accuracy01=0.8604 | loss=0.3256 | lr=4.737e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (valid): accuracy01=0.8670 | loss=0.3169\n",
      "[2020-05-11 15:53:28,170] \n",
      "7/10 * Epoch 7 (_base): lr=3.158e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): accuracy01=0.8676 | loss=0.3119 | lr=3.684e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (valid): accuracy01=0.8716 | loss=0.3109\n",
      "[2020-05-11 15:54:17,693] \n",
      "8/10 * Epoch 8 (_base): lr=2.105e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): accuracy01=0.8727 | loss=0.3041 | lr=2.631e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (valid): accuracy01=0.8732 | loss=0.3073\n",
      "[2020-05-11 15:55:08,444] \n",
      "9/10 * Epoch 9 (_base): lr=1.053e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): accuracy01=0.8748 | loss=0.2978 | lr=1.579e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (valid): accuracy01=0.8754 | loss=0.3045\n",
      "[2020-05-11 15:55:58,091] \n",
      "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): accuracy01=0.8775 | loss=0.2946 | lr=5.260e-07 | momentum=0.9000\n",
      "10/10 * Epoch 10 (valid): accuracy01=0.8751 | loss=0.3035\n",
      "Top best models:\n",
      "logs/checkpoints/train.10.pth\t0.3035\n"
     ]
    }
   ],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader\n",
    "}\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"token_type_ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_labels),\n",
    "        SchedulerCallback(mode='batch'),\n",
    "    ],\n",
    "    logdir=log_dir,\n",
    "    num_epochs=epochs,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.805045871559633"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = runner.predict_loader(model=model, loader=test_dataloader, resume=log_dir+'checkpoints/best.pth')\n",
    "y_pred = []\n",
    "counter = 0\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for i in logits:\n",
    "    for j in softmax(i['logits']).argmax(axis=1).tolist():\n",
    "        y_pred.append(j)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "for i in test_dataloader:\n",
    "    for j in i['targets'].tolist():\n",
    "        y_true.append(j)\n",
    "        \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем google/bert_uncased_L-2_H-128_A-2 на QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 83032: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 154657: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 323916: expected 6 fields, saw 7\\n'\n",
      "/home/care1e55/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning:\n",
      "\n",
      "Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>363177.000000</td>\n",
       "      <td>363177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>220914.009747</td>\n",
       "      <td>0.369354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>159920.121426</td>\n",
       "      <td>0.482631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74634.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>196944.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>354683.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                qid2   is_duplicate\n",
       "count  363177.000000  363177.000000\n",
       "mean   220914.009747       0.369354\n",
       "std    159920.121426       0.482631\n",
       "min         2.000000       0.000000\n",
       "25%     74634.000000       0.000000\n",
       "50%    196944.000000       0.000000\n",
       "75%    354683.000000       1.000000\n",
       "max    537933.000000       1.000000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/QQP/train.tsv\", sep=\"\t\", error_bad_lines=False).dropna()\n",
    "data.describe()\n",
    "# \t363185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201359</td>\n",
       "      <td>303345</td>\n",
       "      <td>303346</td>\n",
       "      <td>Why are African-Americans so beautiful?</td>\n",
       "      <td>Why are hispanics so beautiful?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263843</td>\n",
       "      <td>69383</td>\n",
       "      <td>380476</td>\n",
       "      <td>I want to pursue PhD in Computer Science about...</td>\n",
       "      <td>I handle social media for a non-profit. Should...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172974</td>\n",
       "      <td>266948</td>\n",
       "      <td>175089</td>\n",
       "      <td>Is there a reason why we should travel alone?</td>\n",
       "      <td>What are some reasons to travel alone?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15329</td>\n",
       "      <td>29298</td>\n",
       "      <td>29299</td>\n",
       "      <td>Why are people so obsessed with having a girlf...</td>\n",
       "      <td>How can a single male have a child?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209794</td>\n",
       "      <td>314169</td>\n",
       "      <td>314170</td>\n",
       "      <td>What are some good baby girl names starting wi...</td>\n",
       "      <td>What are some good baby girl names starting wi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  201359  303345  303346            Why are African-Americans so beautiful?   \n",
       "1  263843   69383  380476  I want to pursue PhD in Computer Science about...   \n",
       "2  172974  266948  175089      Is there a reason why we should travel alone?   \n",
       "3   15329   29298   29299  Why are people so obsessed with having a girlf...   \n",
       "4  209794  314169  314170  What are some good baby girl names starting wi...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0                    Why are hispanics so beautiful?           0.0  \n",
       "1  I handle social media for a non-profit. Should...           0.0  \n",
       "2             What are some reasons to travel alone?           1.0  \n",
       "3                How can a single male have a child?           0.0  \n",
       "4  What are some good baby girl names starting wi...           0.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_pd, valid_pd = train_test_split(data, test_size=0.2)\n",
    "test_pd = pd.read_csv(\"./data/QQP/dev.tsv\", sep=\"\\t\").dropna()\n",
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        self.encoded_plus = [ \n",
    "            tokenizer.encode_plus(text=item[0],text_pair=item[1] , max_length=max_seq_length, pad_to_max_length=True) \n",
    "            for item in zip(self.data['question1'], self.data['question2'])\n",
    "        ] \n",
    "        \n",
    "#         print(self.encoded_plus[0])\n",
    "        \n",
    "        self.input_ids = torch.tensor([ i['input_ids'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.attention_mask = torch.tensor([ i['attention_mask'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.token_type_ids = torch.tensor([ i['token_type_ids'] for i in self.encoded_plus ], dtype=torch.long) \n",
    "        self.target = torch.tensor(list(self.data['is_duplicate']), dtype=torch.long) \n",
    "        # </YOUR CODE>\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'token_type_ids': self.token_type_ids[idx],\n",
    "            'targets': self.target[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name: str, num_labels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # <YOUR CODE>\n",
    "        \n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            pretrained_model_name,\n",
    "            num_labels = num_labels\n",
    "        )\n",
    "        \n",
    "        self.bert = transformers.BertModel.from_pretrained(\n",
    "            pretrained_model_name, \n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # </YOUR CODE>\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        \n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        bert_output = self.bert(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        hidden_state = bert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        # <YOUR CODE>\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "        # </YOUR CODE>\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "# <YOUR CODE>\n",
    "pretrained_model_name = 'google/bert_uncased_L-2_H-128_A-2'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
    "# </YOUR CODE>\n",
    "\n",
    "model.to(device)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9080\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# <YOUR CODE>\n",
    "train_dataset = TextClassificationDataset(train_pd, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_dataset = TextClassificationDataset(valid_pd, tokenizer)\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TextClassificationDataset(test_pd, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader,\n",
    "    \"test\": test_dataloader    \n",
    "}\n",
    "# </YOUR CODE>\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 404\n",
    "set_global_seed(seed)\n",
    "prepare_cudnn(True)\n",
    "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "warmup_steps = len(train_dataloader) // 2\n",
    "t_total = len(train_dataloader) * epochs\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# <YOUR CODE>\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "# </YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
      "\n",
      "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 14:31:05,336] \n",
      "1/10 * Epoch 1 (_base): lr=9.474e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): accuracy01=0.7044 | loss=0.5368 | lr=7.369e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (valid): accuracy01=0.7622 | loss=0.4592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
      "\n",
      "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 14:35:28,864] \n",
      "2/10 * Epoch 2 (_base): lr=8.421e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): accuracy01=0.7722 | loss=0.4471 | lr=8.947e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (valid): accuracy01=0.7891 | loss=0.4213\n",
      "[2020-05-11 14:39:54,691] \n",
      "3/10 * Epoch 3 (_base): lr=7.368e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): accuracy01=0.7907 | loss=0.4204 | lr=7.895e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (valid): accuracy01=0.7970 | loss=0.4071\n",
      "[2020-05-11 14:44:20,374] \n",
      "4/10 * Epoch 4 (_base): lr=6.316e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): accuracy01=0.8030 | loss=0.4028 | lr=6.842e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (valid): accuracy01=0.7943 | loss=0.4114\n",
      "[2020-05-11 14:48:45,891] \n",
      "5/10 * Epoch 5 (_base): lr=5.263e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): accuracy01=0.8113 | loss=0.3916 | lr=5.789e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (valid): accuracy01=0.8033 | loss=0.3987\n",
      "[2020-05-11 14:53:09,986] \n",
      "6/10 * Epoch 6 (_base): lr=4.211e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): accuracy01=0.8162 | loss=0.3827 | lr=4.737e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (valid): accuracy01=0.8061 | loss=0.3952\n",
      "[2020-05-11 14:57:34,194] \n",
      "7/10 * Epoch 7 (_base): lr=3.158e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): accuracy01=0.8199 | loss=0.3759 | lr=3.684e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (valid): accuracy01=0.8163 | loss=0.3811\n",
      "[2020-05-11 15:02:00,258] \n",
      "8/10 * Epoch 8 (_base): lr=2.105e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): accuracy01=0.8243 | loss=0.3705 | lr=2.632e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (valid): accuracy01=0.8153 | loss=0.3820\n",
      "[2020-05-11 15:06:23,003] \n",
      "9/10 * Epoch 9 (_base): lr=1.053e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): accuracy01=0.8260 | loss=0.3682 | lr=1.579e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (valid): accuracy01=0.8182 | loss=0.3779\n",
      "[2020-05-11 15:10:47,194] \n",
      "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): accuracy01=0.8268 | loss=0.3661 | lr=5.263e-07 | momentum=0.9000\n",
      "10/10 * Epoch 10 (valid): accuracy01=0.8163 | loss=0.3806\n",
      "Top best models:\n",
      "logs/checkpoints/train.9.pth\t0.3779\n"
     ]
    }
   ],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader\n",
    "}\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"token_type_ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_labels),\n",
    "        SchedulerCallback(mode='batch'),\n",
    "    ],\n",
    "    logdir=log_dir,\n",
    "    num_epochs=epochs,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815659755765277"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = runner.predict_loader(model=model, loader=test_dataloader, resume=log_dir+'checkpoints/best.pth')\n",
    "y_pred = []\n",
    "counter = 0\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for i in logits:\n",
    "    for j in softmax(i['logits']).argmax(axis=1).tolist():\n",
    "        y_pred.append(j)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "for i in test_dataloader:\n",
    "    for j in i['targets'].tolist():\n",
    "        y_true.append(j)\n",
    "        \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем google/bert_uncased_L-4_H-256_A-4 на QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "# <YOUR CODE>\n",
    "pretrained_model_name = 'google/bert_uncased_L-4_H-256_A-4'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
    "# </YOUR CODE>\n",
    "\n",
    "model.to(device)\n",
    "print(\"Success!\")\n",
    "seed = 404\n",
    "set_global_seed(seed)\n",
    "prepare_cudnn(True)\n",
    "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "warmup_steps = len(train_dataloader) // 2\n",
    "t_total = len(train_dataloader) * epochs\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# <YOUR CODE>\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "# </YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
      "\n",
      "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 01:33:30,457] \n",
      "1/10 * Epoch 1 (_base): lr=9.474e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): accuracy01=0.7539 | loss=0.4657 | lr=7.369e-06 | momentum=0.9000\n",
      "1/10 * Epoch 1 (valid): accuracy01=0.8117 | loss=0.3846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
      "\n",
      "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-11 01:49:53,157] \n",
      "2/10 * Epoch 2 (_base): lr=8.421e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): accuracy01=0.8272 | loss=0.3669 | lr=8.947e-06 | momentum=0.9000\n",
      "2/10 * Epoch 2 (valid): accuracy01=0.8467 | loss=0.3322\n",
      "[2020-05-11 02:06:15,264] \n",
      "3/10 * Epoch 3 (_base): lr=7.368e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): accuracy01=0.8466 | loss=0.3324 | lr=7.895e-06 | momentum=0.9000\n",
      "3/10 * Epoch 3 (valid): accuracy01=0.8537 | loss=0.3214\n",
      "[2020-05-11 02:22:37,421] \n",
      "4/10 * Epoch 4 (_base): lr=6.316e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): accuracy01=0.8591 | loss=0.3096 | lr=6.842e-06 | momentum=0.9000\n",
      "4/10 * Epoch 4 (valid): accuracy01=0.8595 | loss=0.3079\n",
      "[2020-05-11 02:39:04,583] \n",
      "5/10 * Epoch 5 (_base): lr=5.263e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): accuracy01=0.8686 | loss=0.2925 | lr=5.789e-06 | momentum=0.9000\n",
      "5/10 * Epoch 5 (valid): accuracy01=0.8651 | loss=0.2985\n",
      "[2020-05-11 02:55:26,707] \n",
      "6/10 * Epoch 6 (_base): lr=4.211e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): accuracy01=0.8754 | loss=0.2800 | lr=4.737e-06 | momentum=0.9000\n",
      "6/10 * Epoch 6 (valid): accuracy01=0.8691 | loss=0.2931\n",
      "[2020-05-11 03:11:48,854] \n",
      "7/10 * Epoch 7 (_base): lr=3.158e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): accuracy01=0.8809 | loss=0.2692 | lr=3.684e-06 | momentum=0.9000\n",
      "7/10 * Epoch 7 (valid): accuracy01=0.8712 | loss=0.2918\n",
      "[2020-05-11 03:28:10,864] \n",
      "8/10 * Epoch 8 (_base): lr=2.105e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): accuracy01=0.8849 | loss=0.2614 | lr=2.632e-06 | momentum=0.9000\n",
      "8/10 * Epoch 8 (valid): accuracy01=0.8722 | loss=0.2905\n",
      "[2020-05-11 03:44:32,924] \n",
      "9/10 * Epoch 9 (_base): lr=1.053e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): accuracy01=0.8874 | loss=0.2563 | lr=1.579e-06 | momentum=0.9000\n",
      "9/10 * Epoch 9 (valid): accuracy01=0.8725 | loss=0.2908\n",
      "[2020-05-11 04:00:57,241] \n",
      "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): accuracy01=0.8902 | loss=0.2516 | lr=5.263e-07 | momentum=0.9000\n",
      "10/10 * Epoch 10 (valid): accuracy01=0.8727 | loss=0.2920\n",
      "Top best models:\n",
      "logs/checkpoints/train.8.pth\t0.2905\n"
     ]
    }
   ],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": valid_dataloader\n",
    "}\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"token_type_ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_labels),\n",
    "        SchedulerCallback(mode='batch'),\n",
    "    ],\n",
    "    logdir=log_dir,\n",
    "    num_epochs=epochs,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696589135765772"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = TextClassificationDataset(test_pd, tokenizer)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "logits = runner.predict_loader(model=model, loader=test_dataloader, resume=log_dir+'checkpoints/best.pth')\n",
    "y_pred = []\n",
    "counter = 0\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for i in logits:\n",
    "    for j in softmax(i['logits']).argmax(axis=1).tolist():\n",
    "        y_pred.append(j)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "for i in test_dataloader:\n",
    "    for j in i['targets'].tolist():\n",
    "        y_true.append(j)\n",
    "        \n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViZwMS2pIkBv"
   },
   "source": [
    "Ииии отчет!\n",
    "\n",
    "Напиши внизу небольшой отчет о проделанной работе. Ожидается сравнение результатов модели с разным количеством голов/слоев на разных датасетах на `test`. Если для оценки качества на датасете используется необычная метрика(не Accuracy или F1), то можно использовать один из них. Было бы круто, если бы вычислялась нужная метрика и она использовалась в отчете.\n",
    "\n",
    "<ТВОЙ ОТЧЕТ>\n",
    "\n",
    "В ходе выполнения дз использовались статьи и документация hugging face, например https://huggingface.co/transformers/main_classes/tokenizer.html. Решались задачи бинарной классификации SST-2 и QQP. Полагая, что классы достаточно сбалансированы, использовалась метрика accuracy. Для QQP в tokenizer подавалась пара пердложений (второе как text_pair), а для SST просто предложение. attention_mask, token_type_ids, input_ids получены из tokenizer (спасибо hugging face за это). Хотелось сначала использовать distiledBert, но в нем нет token_type. Для каждого типа задачи были взяты предобученные Bert'ы модели и токенайзера с 4 и 2 head и 256 и 128 hidden_dim соответвенно, к ним добавлялся линейный классификатор на 2 класса. В результате как и ожидалось в первом случае качество лучше, но дольше обучение и более требовательно к ресурсам. Гиперпараметры - без изменений. Немного пришлось повозиться с DataLoaders, например привести названия переменных в консистентный вид и убрать randomsampler для test.\n",
    "\n",
    "\n",
    "</ТВОЙ ОТЧЕТ>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3PbkgpXbd3j"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task</th>\n",
       "      <th>train acc</th>\n",
       "      <th>train loss</th>\n",
       "      <th>val acc</th>\n",
       "      <th>val loss</th>\n",
       "      <th>test acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/bert_uncased_L-4_H-256_A-4</td>\n",
       "      <td>SST-2</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.8555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google/bert_uncased_L-2_H-128_A-2</td>\n",
       "      <td>SST-2</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.2946</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>0.8050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/bert_uncased_L-2_H-128_A-2</td>\n",
       "      <td>QQP</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.3661</td>\n",
       "      <td>0.8163</td>\n",
       "      <td>0.3806</td>\n",
       "      <td>0.8156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google/bert_uncased_L-4_H-256_A-4</td>\n",
       "      <td>QQP</td>\n",
       "      <td>0.8902</td>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.8696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               model   task train acc train loss val acc  \\\n",
       "0  google/bert_uncased_L-4_H-256_A-4  SST-2    0.9291     0.1825  0.9145   \n",
       "1  google/bert_uncased_L-2_H-128_A-2  SST-2    0.8775     0.2946  0.8751   \n",
       "2  google/bert_uncased_L-2_H-128_A-2    QQP    0.8268     0.3661  0.8163   \n",
       "3  google/bert_uncased_L-4_H-256_A-4    QQP    0.8902     0.2516  0.8727   \n",
       "\n",
       "  val loss test acc  \n",
       "0   0.2261   0.8555  \n",
       "1   0.3035   0.8050  \n",
       "2   0.3806   0.8156  \n",
       "3   0.2920   0.8696  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(columns = [\n",
    "        'model', \n",
    "        'task', \n",
    "        'train acc',\n",
    "        'train loss',\n",
    "        'val acc',\n",
    "        'val loss',\n",
    "        'test acc'\n",
    "    ])\n",
    "report.loc[0] = ['google/bert_uncased_L-4_H-256_A-4', 'SST-2', '0.9291', '0.1825', '0.9145', '0.2261', '0.8555']\n",
    "report.loc[1] = ['google/bert_uncased_L-2_H-128_A-2', 'SST-2', '0.8775', '0.2946', '0.8751', '0.3035', '0.8050']\n",
    "report.loc[2] = ['google/bert_uncased_L-2_H-128_A-2', 'QQP', '0.8268', '0.3661', '0.8163', '0.3806', '0.8156']\n",
    "report.loc[3] = ['google/bert_uncased_L-4_H-256_A-4', 'QQP', '0.8902', '0.2516', '0.8727', '0.2920', '0.8696']\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]BERT.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
