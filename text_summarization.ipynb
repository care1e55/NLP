{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmb8UhIzOnfK"
   },
   "source": [
    "# Text Summarization. Homework\n",
    "\n",
    "Всем привет! Это домашка по суммаризации текста.\n",
    "\n",
    "На семинаре мы рассмотрели базовые модели для суммаризации текста. Попробуйте теперь улучшить два метода: TextRank и Extractive RNN. Задание достаточно большое и требует хорошую фантазию, тут можно эксперементировать во всю.\n",
    "\n",
    "Для сдачи заданий надо получить определенное качество по test-у:\n",
    "\n",
    "- 1 задание: 0.27 BLEU\n",
    "- 2 задание: 0.3 BLEU\n",
    "\n",
    "Если ваш подход пробивает это качество – задание считается пройденным. Плюсом будет описание того, почему вы решили использовать то или иное решение. \n",
    "\n",
    "Датасет: gazeta.ru\n",
    "\n",
    "**P.S.** Возможно, в датасете находятся пустые данные. Проверьте эту гипотезу, и если надо, сделайте предобратоку датасета.\n",
    "\n",
    "\n",
    "`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики МФТИ.`\n",
    "\n",
    "Загрузим датасет и необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqkLTkFRfXvA"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
    "# !wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
    "# !wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXS1sdYZCluU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting razdel\n",
      "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
      "Collecting allennlp\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.6MB 91kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Using cached https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting fasttext\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 88kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting OpenNMT-py\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/c7/b3d9bf9a6a681b10c00aa897650f79d4e7ad8a80317c5cddb6a3ef43540c/OpenNMT_py-1.1.1-py3-none-any.whl (189kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 134kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Using cached https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl\n",
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 126kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 158kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting rouge==0.3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/89/af359c22e1d858e0299d4cc9219f36b504817c9797acad23081247867845/rouge-0.3.1-py3-none-any.whl\n",
      "Collecting summa\n",
      "  Downloading https://files.pythonhosted.org/packages/45/3b/1c7dc435d05aef474c4137328400f1e11787b9bffab1f87a3f160c1fef54/summa-1.2.0.tar.gz (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 242kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting flask>=1.0.2 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl\n",
      "Collecting sqlparse>=0.2.4 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/ee/6e821932f413a5c4b76be9c5936e313e4fc626b33f16e027866e1d60f588/sqlparse-0.3.1-py2.py3-none-any.whl (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 165kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jsonpickle (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
      "Collecting spacy<2.2,>=2.1.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 30.9MB 44kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpydoc>=0.8.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
      "Collecting conllu==1.3.1 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
      "Collecting requests>=2.18 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\n",
      "Collecting flaky (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
      "Collecting h5py (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting gevent>=1.3.6 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/f0/46938b0c3e8dd9cfa94699d30427343bfd8bf095079cf2084d1acbed7d2b/gevent-20.5.0.tar.gz (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.3MB 88kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.3 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting matplotlib>=2.2.3 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting unidecode (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 109kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting parsimonious>=0.8.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 81kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting tqdm>=4.19 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl\n",
      "Collecting word2number>=1.1 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 103kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-transformers==1.1.0 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 224kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pytest (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 92kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 166kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboardX>=1.2 (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl\n",
      "Collecting flask-cors>=3.0.7 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting scipy (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting editdistance (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 113kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting overrides (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
      "Collecting ftfy (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    100% |████████████████████████████████| 61kB 120kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting boto3 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/40/31/af60fcf46b8b41b60c7f91fbafa45095ec525c6a6bcd1dc6b82fb7b419eb/boto3-1.13.13-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 116kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting responses>=0.7 (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/01/0c/e4da4191474e27bc41bedab2bf249b27d9261db749f59769d7e7ca8feead/responses-0.10.14-py2.py3-none-any.whl\n",
      "Collecting numpy (from allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scikit-learn (from allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.9MB 97kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from torch)\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl\n",
      "Collecting setuptools>=0.7.0 (from fasttext)\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/c5/0d38afb961f83e0d51f319f7dc166195ebabc1ea3cb20a10a77f500f7156/setuptools-46.4.0-py3-none-any.whl (583kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 110kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six (from OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Collecting configargparse (from OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 159kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard>=1.14 (from OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl\n",
      "Collecting waitress (from OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/ca/ede3ed29723ca944f6e77bd1d7b38c271dd801c7d6a11ab6037597e4fd5b/waitress-1.4.3-py2.py3-none-any.whl (148kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 207kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting torchtext==0.4.0 (from OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 142kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml (from OpenNMT-py)\n",
      "Collecting pyonmttok==1.*; platform_system == \"Linux\" (from OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/20/3c57198ffe690b580fbf23d33d5000eb411862e60e4bb6853b61dc989187/pyonmttok-1.18.3-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 77kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting decorator>=4.3.0 (from networkx)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 58kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting click (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl (298kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 127kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/07/fb11080a1324bc8d7b68deb009a4c08bd675e0789a213028c58323c4aaab/regex-2020.5.14-cp36-cp36m-manylinux1_x86_64.whl (675kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 139kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2>=2.10.1 (from flask>=1.0.2->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl\n",
      "Collecting itsdangerous>=0.24 (from flask>=1.0.2->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting Werkzeug>=0.15 (from flask>=1.0.2->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata (from jsonpickle->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/e4/891bfcaf868ccabc619942f27940c77a8a4b45fd8367098955bb7e152fb1/importlib_metadata-1.6.0-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 85kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting blis<0.3.0,>=0.2.2 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 95kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.2.0 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\n",
      "Collecting thinc<7.1.0,>=7.0.8 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 164kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=0.0.6 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/0e/9a/70bd934dd4d25545c9aa6c8cd4edbac2a33ba9c915439a9209b69f0ec0ad/srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 205kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy<2.2,>=2.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/b5/3e1714ebda8fd7c5859f9b216e381adc0a38b962f071568fd00d67e1b1ca/cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting sphinx>=1.6.5 (from numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/57/ee/b1ca567be96185ccc7d352dffa46cb047eb62fbf437e643d4dee5df09619/Sphinx-3.0.3-py3-none-any.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 132kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests>=2.18->allennlp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests>=2.18->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.18->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/2b/26e37a4b034800c960a00c4e1b3d9ca5d7014e983e6e729e33ea2f36426c/certifi-2020.4.5.1-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests>=2.18->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting greenlet>=0.4.14 (from gevent>=1.3.6->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 104kB/s a 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib>=2.2.3->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib>=2.2.3->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib>=2.2.3->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib>=2.2.3->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting sentencepiece (from pytorch-transformers==1.1.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 160kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting more-itertools>=4.0.0 (from pytest->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/06/b1/2dcadc4861c505a807d5c6d88928450fe5afcf352f205432572a10d74657/more_itertools-8.3.0-py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 223kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from pytest->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from pytest->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/d5/1ecdac957e3ea12c1b319fcdee8b6917ffaff8b4644d673c4d72d2f20b49/wcwidth-0.1.9-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 121kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.4.0 (from pytest->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/db/4313ab3be961f7a763066401fb77f7748373b6094076ae2bda2806988af6/attrs-19.3.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.8.0 (from tensorboardX>=1.2->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6c/d1cf8521f94b332851cf95dbf679a72271ba85b367e2283b9f5f6cdf98bc/protobuf-3.12.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 132kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting botocore<1.17.0,>=1.16.13 (from boto3->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/75/702ebb21a4540187b9edadbb15a7cc43fb5f2d5cb7da359d086f22905c9e/botocore-1.16.13-py2.py3-none-any.whl (6.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.2MB 83kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
      "Collecting grpcio>=1.24.3 (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/23/62d3e82fa4c505f3195315c8a774b2e656b556d174329aa98edb829e48bc/grpcio-1.29.0.tar.gz (19.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 19.6MB 67kB/s eta 0:00:01   41% |█████████████▍                  | 8.2MB 163kB/s eta 0:01:10\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/cd/a0c1f9e4582ea64dddf76c1b808b318d01e3b858a51c715bffab1016ecc7/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/a3/2695fdcda305ea85c43ebd2a4d1429f3c7e897c7cf9045a8c378e1115a15/google_auth-1.15.0-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 128kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting wheel>=0.26; python_version >= \"3\" (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\n",
      "Collecting absl-py>=0.4 (from tensorboard>=1.14->OpenNMT-py)\n",
      "Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask>=1.0.2->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata->jsonpickle->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Collecting sphinxcontrib-applehelp (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/47/86022665a9433d89a66f5911b558ddff69861766807ba685de2e324bd6ed/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 100kB/s a 0:00:01\n",
      "\u001b[?25hCollecting alabaster<0.8,>=0.7 (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ad/00b090d23a222943eb0eda509720a404f531a439e803f6538f35136cae9e/alabaster-0.7.12-py2.py3-none-any.whl\n",
      "Collecting sphinxcontrib-qthelp (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/14/05f9206cf4e9cfca1afb5fd224c7cd434dcc3a433d6d9e4e0264d29c6cdb/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 95kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting sphinxcontrib-serializinghtml (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/9a/ca/bfad79b79b3821d0c6361c431f0ef4aec16ee248338b2c2013008b34d345/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 95kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pygments>=2.0 (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl\n",
      "Collecting sphinxcontrib-jsmath (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/42/4c8646762ee83602e3fb3fbe774c2fac12f317deb0b5dbeeedd2d3ba4b77/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl\n",
      "Collecting babel>=1.3 (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/15/a1/522dccd23e5d2e47aed4b6a16795b8213e3272c7506e625f2425ad025a19/Babel-2.8.0-py2.py3-none-any.whl\n",
      "Collecting sphinxcontrib-devhelp (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/09/5de5ed43a521387f18bdf5f5af31d099605c992fd25372b2b9b825ce48ee/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 136kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting imagesize (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/b2/b5522a0c8d11e4aff83f8342f3f0dea68c2fb25aa44403e420587f0ce204/imagesize-1.2.0-py2.py3-none-any.whl\n",
      "Collecting snowballstemmer>=1.1 (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/4b/cdf1113a0e88b641893b814e9c36f69a6fda28cd88b62c7f0d858cde3166/snowballstemmer-2.0.0-py2.py3-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 199kB/s a 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.12 (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/44/8a15e45ffa96e6cf82956dd8d7af9e666357e16b0d93b253903475ee947f/docutils-0.16-py2.py3-none-any.whl\n",
      "Collecting sphinxcontrib-htmlhelp (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/62/8222554b29b3acde8420128d6d3999c5904d40922ef4b6ccb370e2be7421/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl (96kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 104kB/s  0:00:011\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: fasttext, nltk, summa, numpydoc, gevent, parsimonious, word2number, jsonnet, overrides, ftfy, configargparse, docopt, grpcio\n",
      "  Running setup.py bdist_wheel for fasttext ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "  Running setup.py bdist_wheel for summa ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/6a/09/68/e2f2861c01d86407c3fa5220826ed7eed2abaa56b001be5970\n",
      "  Running setup.py bdist_wheel for numpydoc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
      "  Running setup.py bdist_wheel for gevent ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/9b/5a/31/354896198784d0475a7a1819e63efdb8e9e50822f172695b83\n",
      "  Running setup.py bdist_wheel for parsimonious ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
      "  Running setup.py bdist_wheel for word2number ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
      "  Running setup.py bdist_wheel for jsonnet ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n",
      "  Running setup.py bdist_wheel for overrides ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
      "  Running setup.py bdist_wheel for configargparse ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
      "  Running setup.py bdist_wheel for docopt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/ed/06/79/e559ab3b10134903b88e2df2df1b7cc4d3f1a92a46972a09fb\n",
      "Successfully built fasttext nltk summa numpydoc gevent parsimonious word2number jsonnet overrides ftfy configargparse docopt grpcio\n",
      "Installing collected packages: razdel, click, joblib, regex, tqdm, nltk, MarkupSafe, Jinja2, itsdangerous, Werkzeug, flask, sqlparse, zipp, importlib-metadata, jsonpickle, murmurhash, cymem, preshed, numpy, blis, wasabi, srsly, plac, thinc, urllib3, idna, certifi, chardet, requests, spacy, sphinxcontrib-applehelp, alabaster, sphinxcontrib-qthelp, pyparsing, six, packaging, sphinxcontrib-serializinghtml, Pygments, sphinxcontrib-jsmath, pytz, babel, sphinxcontrib-devhelp, imagesize, snowballstemmer, docutils, sphinxcontrib-htmlhelp, setuptools, sphinx, numpydoc, conllu, flaky, h5py, greenlet, gevent, cycler, kiwisolver, python-dateutil, matplotlib, unidecode, parsimonious, word2number, future, torch, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, sentencepiece, pytorch-transformers, more-itertools, pluggy, wcwidth, py, attrs, pytest, jsonnet, protobuf, tensorboardX, flask-cors, scipy, editdistance, overrides, ftfy, responses, threadpoolctl, scikit-learn, allennlp, pybind11, fasttext, configargparse, grpcio, tensorboard-plugin-wit, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, wheel, markdown, absl-py, tensorboard, waitress, torchtext, pyyaml, pyonmttok, OpenNMT-py, decorator, networkx, pymorphy2-dicts, dawg-python, docopt, pymorphy2, rouge, summa\n",
      "Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 OpenNMT-py-1.1.1 Pygments-2.6.1 Werkzeug-1.0.1 absl-py-0.9.0 alabaster-0.7.12 allennlp-0.9.0 attrs-19.3.0 babel-2.8.0 blis-0.2.4 boto3-1.13.13 botocore-1.16.13 cachetools-4.1.0 certifi-2020.4.5.1 chardet-3.0.4 click-7.1.2 configargparse-1.2.3 conllu-1.3.1 cycler-0.10.0 cymem-2.0.3 dawg-python-0.7.2 decorator-4.4.2 docopt-0.6.2 docutils-0.16 editdistance-0.5.3 fasttext-0.9.2 flaky-3.6.1 flask-1.1.2 flask-cors-3.0.8 ftfy-5.7 future-0.18.2 gevent-20.5.0 google-auth-1.15.0 google-auth-oauthlib-0.4.1 greenlet-0.4.15 grpcio-1.29.0 h5py-2.10.0 idna-2.9 imagesize-1.2.0 importlib-metadata-1.6.0 itsdangerous-1.1.0 jmespath-0.10.0 joblib-0.15.1 jsonnet-0.15.0 jsonpickle-1.4.1 kiwisolver-1.2.0 markdown-3.2.2 matplotlib-3.2.1 more-itertools-8.3.0 murmurhash-1.0.2 networkx-2.4 nltk-3.5 numpy-1.18.4 numpydoc-0.9.2 oauthlib-3.1.0 overrides-3.0.0 packaging-20.4 parsimonious-0.8.1 plac-0.9.6 pluggy-0.13.1 preshed-2.0.1 protobuf-3.12.0 py-1.8.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pybind11-2.5.0 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pyonmttok-1.18.3 pyparsing-2.4.7 pytest-5.4.2 python-dateutil-2.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 pytz-2020.1 pyyaml-5.3.1 razdel-0.5.0 regex-2020.5.14 requests-2.23.0 requests-oauthlib-1.3.0 responses-0.10.14 rouge-0.3.1 rsa-4.0 s3transfer-0.3.3 scikit-learn-0.23.1 scipy-1.4.1 sentencepiece-0.1.90 setuptools-46.4.0 six-1.14.0 snowballstemmer-2.0.0 spacy-2.1.9 sphinx-3.0.3 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-1.0.3 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.4 sqlparse-0.3.1 srsly-1.0.2 summa-1.2.0 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorboardX-2.0 thinc-7.0.8 threadpoolctl-2.0.0 torch-1.5.0 torchtext-0.4.0 tqdm-4.46.0 unidecode-1.1.1 urllib3-1.25.9 waitress-1.4.3 wasabi-0.6.0 wcwidth-0.1.9 wheel-0.34.2 word2number-1.1 zipp-3.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 137kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting youtokentome\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/ae/f8b0d15696766eb35dda6cf84a23d42ae7f3ba37aa30e5e2287fd94ac053/youtokentome-1.0.6.tar.gz (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 102kB/s a 0:00:01\n",
      "\u001b[?25hCollecting catalyst\n",
      "  Using cached https://files.pythonhosted.org/packages/64/05/ac1e89cd9533c66e10e286d23dc2c4d042abee177bba9d1be738f55fa10d/catalyst-20.5-py2.py3-none-any.whl\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting sacremoses (from transformers)\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl\n",
      "Collecting tokenizers==0.7.0 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/07/fb11080a1324bc8d7b68deb009a4c08bd675e0789a213028c58323c4aaab/regex-2020.5.14-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
      "Collecting numpy (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting requests (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\n",
      "Collecting Click>=7.0 (from youtokentome)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting ipython (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/00/afc3968a3cdf5f30c5c9dfb8e6a61e63231d6869a461dc1ff418280c5ea4/ipython-7.14.0-py3-none-any.whl\n",
      "Collecting torch>=1.1.0 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tensorboard>=1.14.0 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl\n",
      "Collecting matplotlib (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scikit-learn>=0.20 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting GitPython>=3.1.1 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl\n",
      "Collecting packaging (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting deprecation (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
      "Collecting plotly>=4.1.0 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/78/eb6cbe96c8379c54819592bb228c58ed7386fcc60a55eca7db99432fdf14/plotly-4.7.1-py2.py3-none-any.whl\n",
      "Collecting pandas>=0.22 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/71/8f53bdbcbc67c912b888b40def255767e475402e9df64050019149b1a943/pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting crc32c>=1.7 (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/82/f60248c01a8a23ae07bd4c43d78d69b20ffe324311db3b0785e391aa09d2/crc32c-2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tensorboardX (from catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl\n",
      "Collecting PyYAML (from catalyst)\n",
      "Collecting six (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/2b/26e37a4b034800c960a00c4e1b3d9ca5d7014e983e6e729e33ea2f36426c/certifi-2020.4.5.1-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\n",
      "Collecting pickleshare (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting backcall (from ipython->catalyst)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/e4/a7/81b39aa50e9284fe2cb21cc7fb7de7817b224172d42793fd57451d38842b/prompt_toolkit-3.0.5-py3-none-any.whl\n",
      "Collecting decorator (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting pygments (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n",
      "Collecting jedi>=0.10 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/3f/67f027e18c60a800875df1a0894a2436ce9053637fa39725766e937c0a71/jedi-0.17.0-py2.py3-none-any.whl\n",
      "Collecting traitlets>=4.2 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl\n",
      "Collecting setuptools>=18.5 (from ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/c5/0d38afb961f83e0d51f319f7dc166195ebabc1ea3cb20a10a77f500f7156/setuptools-46.4.0-py3-none-any.whl\n",
      "Collecting future (from torch>=1.1.0->catalyst)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\n",
      "Collecting absl-py>=0.4 (from tensorboard>=1.14.0->catalyst)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/a3/2695fdcda305ea85c43ebd2a4d1429f3c7e897c7cf9045a8c378e1115a15/google_auth-1.15.0-py2.py3-none-any.whl\n",
      "Collecting wheel>=0.26; python_version >= \"3\" (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/cd/a0c1f9e4582ea64dddf76c1b808b318d01e3b858a51c715bffab1016ecc7/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.0 (from tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6c/d1cf8521f94b332851cf95dbf679a72271ba85b367e2283b9f5f6cdf98bc/protobuf-3.12.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting grpcio>=1.24.3 (from tensorboard>=1.14.0->catalyst)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cycler>=0.10 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
      "Collecting scipy>=0.19.1 (from scikit-learn>=0.20->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1.1->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl\n",
      "Collecting retrying>=1.3.3 (from plotly>=4.1.0->catalyst)\n",
      "Collecting pytz>=2017.2 (from pandas>=0.22->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/d5/1ecdac957e3ea12c1b319fcdee8b6917ffaff8b4644d673c4d72d2f20b49/wcwidth-0.1.9-py2.py3-none-any.whl\n",
      "Collecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting parso>=0.7.0 (from jedi>=0.10->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b5/61/998cce9e7476de000d031874df26a18f67cb73448164fc44a98f0c55920b/parso-0.7.0-py2.py3-none-any.whl\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/e4/891bfcaf868ccabc619942f27940c77a8a4b45fd8367098955bb7e152fb1/importlib_metadata-1.6.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl\n",
      "Collecting smmap<4,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.1.1->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: youtokentome\n",
      "  Running setup.py bdist_wheel for youtokentome ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/care1e55/.cache/pip/wheels/52/f0/38/3804edc705a45a8094e958f3ef0c2dab3e616ef4f91df1981b\n",
      "Successfully built youtokentome\n",
      "Installing collected packages: filelock, six, joblib, tqdm, regex, Click, sacremoses, sentencepiece, tokenizers, dataclasses, numpy, certifi, urllib3, chardet, idna, requests, transformers, youtokentome, pickleshare, backcall, wcwidth, prompt-toolkit, decorator, pygments, ptyprocess, pexpect, parso, jedi, ipython-genutils, traitlets, setuptools, ipython, future, torch, zipp, importlib-metadata, markdown, absl-py, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, wheel, tensorboard-plugin-wit, werkzeug, protobuf, grpcio, tensorboard, kiwisolver, cycler, python-dateutil, pyparsing, matplotlib, threadpoolctl, scipy, scikit-learn, smmap, gitdb, GitPython, packaging, deprecation, retrying, plotly, pytz, pandas, crc32c, tensorboardX, PyYAML, catalyst\n",
      "Successfully installed Click-7.1.2 GitPython-3.1.2 PyYAML-5.3.1 absl-py-0.9.0 backcall-0.1.0 cachetools-4.1.0 catalyst-20.5 certifi-2020.4.5.1 chardet-3.0.4 crc32c-2.0 cycler-0.10.0 dataclasses-0.7 decorator-4.4.2 deprecation-2.1.0 filelock-3.0.12 future-0.18.2 gitdb-4.0.5 google-auth-1.15.0 google-auth-oauthlib-0.4.1 grpcio-1.29.0 idna-2.9 importlib-metadata-1.6.0 ipython-7.14.0 ipython-genutils-0.2.0 jedi-0.17.0 joblib-0.15.1 kiwisolver-1.2.0 markdown-3.2.2 matplotlib-3.2.1 numpy-1.18.4 oauthlib-3.1.0 packaging-20.4 pandas-1.0.3 parso-0.7.0 pexpect-4.8.0 pickleshare-0.7.5 plotly-4.7.1 prompt-toolkit-3.0.5 protobuf-3.12.0 ptyprocess-0.6.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.6.1 pyparsing-2.4.7 python-dateutil-2.8.1 pytz-2020.1 regex-2020.5.14 requests-2.23.0 requests-oauthlib-1.3.0 retrying-1.3.3 rsa-4.0 sacremoses-0.0.43 scikit-learn-0.23.1 scipy-1.4.1 sentencepiece-0.1.90 setuptools-46.4.0 six-1.14.0 smmap-3.0.4 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorboardX-2.0 threadpoolctl-2.0.0 tokenizers-0.7.0 torch-1.5.0 tqdm-4.46.0 traitlets-4.3.3 transformers-2.9.1 urllib3-1.25.9 wcwidth-0.1.9 werkzeug-1.0.1 wheel-0.34.2 youtokentome-1.0.6 zipp-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --upgrade razdel allennlp torch fasttext OpenNMT-py networkx pymorphy2 nltk rouge==0.3.1 summa\n",
    "# !pip3 install transformers youtokentome catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pZ2UGS2DGjH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(eval(line)) # Simple hack\n",
    "    records = pd.DataFrame(records)\n",
    "    if sort_by_date:\n",
    "        records = records.sort(\"date\")\n",
    "    if shuffle:\n",
    "        records = records.sample(frac=1)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNDp-BunEA91"
   },
   "outputs": [],
   "source": [
    "train_records = read_gazeta_records(\"gazeta_train.txt\")\n",
    "val_records = read_gazeta_records(\"gazeta_val.txt\")\n",
    "test_records = read_gazeta_records(\"gazeta_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsAcVSli3r3S"
   },
   "source": [
    "## 1 задание: TextRank (порог: 0.27 BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7jAQp-_Ds98"
   },
   "source": [
    "TextRank - unsupervised метод для составления кратких выжимок из текста. \n",
    "Описание метода:\n",
    "\n",
    "1. Сплитим текст по предложениям\n",
    "2. Считаем \"похожесть\" предложений между собой\n",
    "3. Строим граф предложений с взвешенными ребрами\n",
    "4. С помощью алгоритм PageRank получаем наиболее важные предложения, на основе которых делаем summary.\n",
    "\n",
    "Функция похожести можно сделать и из нейросетевых(или около) моделек: FastText, ELMO и BERT. Выберете один метод, загрузите предобученную модель и с ее помощью для каждого предложениия сделайте sentence embedding. С помощью косинусной меры определяйте похожесть предложений.\n",
    "\n",
    "Предобученные модели можно взять по [ссылке](http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "def calc_scores(references, predictions, metric=\"all\"):\n",
    "    print(\"Count:\", len(predictions))\n",
    "    print(\"Ref:\", references[-1])\n",
    "    print(\"Hyp:\", predictions[-1])\n",
    "\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        print(\"ROUGE: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: из-за того, что кредиторы стали оценивать заемщиков по долговой нагрузке, увеличилось число заявок на займы в микрофинансовых организациях. спрос за десять месяцев вырос на 20% по сравнению с прошлым годом. при этом одобряются отнюдь не все заявки, и часть россиян пойдут к нелегальным кредиторам, чтобы перехватить денег до зарплаты, предупреждают эксперты.\n",
      "Hyp: спрос россиян на займы в мфо растет. по данным на 1 ноября этого года, наши сограждане оформили 170 млн заявок на получение займов в микрофинансовых организациях. об этом рассказал «газете.ru» генеральный директор финансового маркета юником24 юрий кудряков. это выше на 20%, чем в соответствующий период прошлого года. так, годом ранее россияне отправили около 141 млн заявок за 10 месяцев, в 2017 году — 107-110 млн. в целом эксперт считает, что по итогам этого года физические лица подадут порядка 200 млн заявок на получение займов. при этом реально будет выдано всего 32-33 млн займов. увеличившийся спрос на займы вызван ужесточениями на рынке кредитования, которые ввел осенью регулятор. так, с 1 октября финансовая организация, принимая решение о выдаче займа или кредита, обязана учитывать показатель долговой нагрузки (пдн) клиента. банки и мфо обязаны рассчитывать пдн по займам от 10 тысяч рублей как отношение ежемесячных платежей по текущим кредитам к ежемесячному доходу заемщика. при этом никакого норматива, соблюдать который кредиторы должны начать «прямо сегодня», не было введено. каждая финансовая организация должна самостоятельно определять приемлемый уровень долговой нагрузки. однако опрошенные «газетой.ru» эксперты отмечали, что нагрузка выше 50% уже может считаться заградительным барьером для кредитования. показатель долговой нагрузки был введен для того, чтобы не допустить появления «кредитного пузыря», о котором беспокоились чиновники. так, центробанк не раз указывал, что высокие темпы роста необеспеченного потребительского кредитования сегмента (на 1 сентября оно составило 23,8%) несут риски для финансовой стабильности. при этом минэкономразвития так и вовсе заявляло о формировании кредитного пузыря в российской экономике и допускало, что он может привести к рецессии в экономике. но не брать деньги россияне не могут. по словам президента сро напка эльмана мехтиева, наши сограждане остро нуждается в заемных средствах.\n",
      "BLEU:  0.19863467066557175\n",
      "ROUGE:  {'rouge-1': {'f': 0.23990888032001306, 'p': 0.22210993456361303, 'r': 0.3762674667916425}, 'rouge-2': {'f': 0.10326102839824888, 'p': 0.09781012595299302, 'r': 0.16229710223359023}, 'rouge-l': {'f': 0.1863037054780851, 'p': 0.20166026913435278, 'r': 0.34680246323265956}}\n"
     ]
    }
   ],
   "source": [
    "import razdel\n",
    "\n",
    "def calc_lead_n_score(records, n=3, lower=True, nrows=1000):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for text, summary in records[['text', 'summary']].values[:nrows]:\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        text = text if not lower else text.lower()\n",
    "        sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
    "        prediction = \" \".join(sentences[:n])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "calc_lead_n_score(test_records, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/care1e55/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/care1e55/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/care1e55/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/care1e55/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.common.file import read_json\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "bert_config = read_json(configs.embedder.bert_embedder)\n",
    "bert_config['metadata']['variables']['BERT_PATH'] = '/storage/Study/neuro2/hw5/rubert_cased_L-12_H-768_A-12_pt'\n",
    "\n",
    "m = build_model(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vec(words, embeddings, dim=768):\n",
    "    n_known = 0\n",
    "    result = np.array([0] * dim, dtype=float)\n",
    "    \n",
    "    for word in words:\n",
    "        result += embeddings([word])[1][0][0]\n",
    "        n_known += 1\n",
    "\n",
    "    if n_known != 0:\n",
    "        return result/n_known\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.63580272e-02, -2.99739808e-01,  4.27470952e-01,  2.82085747e-01,\n",
       "        2.20809788e-01,  6.00778222e-01, -2.38174558e-01, -4.78468537e-01,\n",
       "       -8.75081941e-02,  8.52746725e-01,  1.19910038e+00, -2.81395972e-01,\n",
       "       -3.30129862e-01,  3.23548526e-01,  3.03210706e-01,  8.76909316e-01,\n",
       "       -1.57103479e-01, -3.42637263e-02, -4.50852215e-01,  1.12513840e+00,\n",
       "       -4.93100941e-01,  1.19064689e-01,  1.50534928e-01, -7.53967583e-01,\n",
       "        1.47298753e-01,  8.76324594e-01,  6.48700237e-01,  2.52484083e-01,\n",
       "        9.94792655e-02, -5.42144299e-01,  3.85844767e-01,  4.96216118e-01,\n",
       "       -5.40555298e-01,  8.89666915e-01,  4.15382951e-01, -1.82250619e-01,\n",
       "        4.37541902e-01,  7.66776204e-01, -1.10241091e+00, -9.74568546e-01,\n",
       "        8.56557041e-02, -3.52700233e-01, -9.17057320e-02,  3.38391542e-01,\n",
       "       -6.72827065e-02, -5.42805158e-02,  5.43608665e-01, -1.43660262e-01,\n",
       "       -1.20182073e+00,  1.39885455e-01,  1.25244766e-01,  1.83576524e-01,\n",
       "        1.03011835e+00, -1.27260640e-01,  6.48427069e-01,  7.85422772e-02,\n",
       "        8.38678256e-02, -5.78358650e-01,  3.37550521e-01, -7.05280006e-01,\n",
       "        2.10234761e-01,  5.60736418e-01, -1.02874950e-01,  3.76147628e-01,\n",
       "       -1.57563537e-01,  1.02138901e+00, -7.33989477e-01, -7.68554568e-01,\n",
       "        3.08207840e-01, -3.85878742e-01,  1.13993323e+00,  5.92521608e-01,\n",
       "        3.06312799e-01, -2.42947936e-01, -1.46577388e-01, -8.61810520e-02,\n",
       "        5.36773503e-01,  2.36540418e-02,  3.14624071e-01,  3.89373004e-01,\n",
       "        1.00938010e+00,  3.22201937e-01,  2.15230323e-03,  8.09238479e-02,\n",
       "       -2.22296849e-01,  3.90288621e-01,  1.35342523e-01, -1.02508262e-01,\n",
       "        7.45830119e-01, -2.99461275e-01, -8.67734671e-01, -6.62297487e-01,\n",
       "        7.36909568e-01, -9.71811339e-02,  2.02613056e-01,  6.17297053e-01,\n",
       "       -9.01666507e-02,  6.63631797e-01,  6.00773990e-01,  6.74717650e-02,\n",
       "        1.07024364e-01, -5.12305975e-01,  3.08437228e-01, -7.14563429e-01,\n",
       "        1.26667523e+00, -1.35735884e-01, -8.30240130e-01, -5.56491971e-01,\n",
       "       -3.22062075e-01, -1.11651182e-01,  6.11736417e-01, -2.17809960e-01,\n",
       "        4.95895803e-01, -4.92446199e-02, -3.44464928e-01, -1.68664083e-01,\n",
       "       -6.84129775e-01,  4.99680787e-01,  5.11510372e-01, -4.47483778e-01,\n",
       "        5.89524984e-01,  7.83128315e-04, -7.69982696e-01, -1.97370037e-01,\n",
       "       -7.72933185e-01,  3.07832867e-01, -2.92406887e-01, -5.76637030e-01,\n",
       "        4.81741518e-01, -8.19280207e-01,  4.45508629e-01,  9.58413817e-03,\n",
       "        2.42157906e-01, -6.00989640e-01, -4.90603715e-01, -4.34238106e-01,\n",
       "        6.10022843e-01,  4.80795532e-01, -3.27536575e-02,  5.40920436e-01,\n",
       "       -3.79816927e-02,  8.77514929e-02,  4.35769707e-02,  6.49842858e-01,\n",
       "       -2.48128772e-01, -2.79399157e-01, -5.62535524e-01, -1.23206683e-01,\n",
       "        1.06445742e+00, -3.40356112e-01, -6.44634813e-02,  2.06479251e-01,\n",
       "        1.34373695e-01, -1.19339198e-01, -1.30026296e-01,  6.74640238e-02,\n",
       "       -3.29398990e-01,  2.12021336e-01,  1.35768786e-01,  1.39918238e-01,\n",
       "        1.72041431e-01,  4.18216974e-01,  7.20123053e-01,  3.04621607e-01,\n",
       "       -1.34991348e+00, -3.88690412e-01,  5.42865060e-02,  1.21148162e-01,\n",
       "        7.53440335e-02,  3.67975503e-01, -6.61757529e-01,  3.89296383e-01,\n",
       "       -1.08188897e-01,  3.77198935e-01, -8.57775584e-02,  7.28638053e-01,\n",
       "        3.75135422e-01,  1.80472225e-01, -5.14737606e-01, -5.19401789e-01,\n",
       "        4.77278829e-02,  1.13945656e-01, -7.29120225e-02,  4.04107869e-01,\n",
       "       -2.30818674e-01,  2.50816286e-01,  3.08131486e-01, -3.88587475e-01,\n",
       "       -6.12096667e-01,  2.20481053e-01, -1.74239889e-01,  2.22382486e-01,\n",
       "       -6.49995744e-01, -6.81199133e-01, -9.41028655e-01,  2.12891437e-02,\n",
       "       -4.40811455e-01,  2.26650655e-01, -3.13278101e-02, -5.30135453e-01,\n",
       "       -7.13896990e-01, -1.62908658e-01, -7.92672113e-02, -2.43927136e-01,\n",
       "       -1.14570841e-01, -9.23034370e-01, -1.09090716e-01, -6.44112051e-01,\n",
       "       -5.42500973e-01,  1.50172377e+00,  2.67906040e-01, -4.69411314e-01,\n",
       "       -8.01416278e-01,  5.12860715e-01,  5.29274404e-01, -1.16152205e-01,\n",
       "       -1.72034636e-01, -2.81642616e-01,  5.22879779e-01,  2.60973841e-01,\n",
       "        9.11227092e-02, -5.68351924e-01, -4.36900854e-01, -4.09199148e-01,\n",
       "       -1.02527872e-01,  4.42527980e-03, -1.58687294e-01,  1.32292598e-01,\n",
       "       -3.72201949e-02, -8.38592589e-01,  6.18249834e-01, -2.32023105e-01,\n",
       "        2.75022060e-01, -5.28364964e-02,  2.45311800e-02,  6.97295129e-01,\n",
       "       -8.19161355e-01, -1.19673036e-01, -1.99395105e-01, -3.64382535e-01,\n",
       "        1.28204596e+00,  2.20849559e-01,  1.45507921e-02, -8.19404051e-02,\n",
       "        4.55407560e-01,  1.74513593e-01, -8.61786604e-02, -1.89229056e-01,\n",
       "        2.71981448e-01,  1.39451757e-01, -1.54370832e+00, -5.21908775e-02,\n",
       "        3.03790361e-01, -2.05447599e-01,  2.21391723e-01,  8.64520669e-01,\n",
       "       -2.29560345e-01, -1.49608925e-01, -3.22535962e-01,  7.96589777e-02,\n",
       "        7.49985516e-01, -1.20198178e+00, -7.28689492e-01,  4.93231535e-01,\n",
       "       -1.20575547e+00,  9.16913390e-01,  1.89593703e-01,  5.21183312e-01,\n",
       "       -2.64302462e-01,  3.40864927e-01, -7.08503723e-01,  4.23573047e-01,\n",
       "        2.50047386e-01,  7.48271108e-01, -4.49840546e-01, -5.77712297e-01,\n",
       "        3.60127062e-01,  4.04624194e-01, -6.51655972e-01, -1.70100987e-01,\n",
       "        3.50355655e-01, -5.05075455e-01, -1.58215240e-01, -9.47794318e-01,\n",
       "       -6.73395157e-01,  5.27439788e-02, -9.24465954e-02,  1.39476225e-01,\n",
       "        4.00389880e-01,  2.53610939e-01, -1.24888808e-01, -4.21452940e-01,\n",
       "       -2.94942528e-01,  2.38415092e-01, -2.23677635e-01, -3.90570939e-01,\n",
       "       -1.13249063e+00,  2.85224617e-01, -6.53862298e-01,  7.85117924e-01,\n",
       "        3.17396641e-01,  6.87062368e-02, -5.59527874e-01, -1.31367266e-01,\n",
       "       -5.82524538e-01, -2.24759847e-01, -2.26753727e-01, -4.26867068e-01,\n",
       "        5.82976878e-01, -3.81015241e-01, -5.09158194e-01,  3.05279464e-01,\n",
       "       -2.07074255e-01,  2.33999386e-01,  2.02124551e-01,  1.50566861e-01,\n",
       "       -9.68950689e-02,  2.29453489e-01, -2.31589869e-01,  3.62601250e-01,\n",
       "       -6.27063394e-01, -8.59268829e-02, -4.68793660e-01, -2.65589595e-01,\n",
       "       -3.42418551e-01, -2.32484356e-01, -4.40492988e-01,  1.47872092e-02,\n",
       "        2.46688306e-01,  3.41004372e-01, -1.62733734e-01,  8.80571723e-01,\n",
       "       -2.64825821e-01, -3.87813926e-01, -7.36505538e-02,  1.37339056e-01,\n",
       "        1.10021167e-01, -1.08104177e-01, -4.86248136e-01, -9.27586436e-01,\n",
       "       -1.21062815e-01,  1.52713880e-01,  7.46993005e-01, -2.44796206e-03,\n",
       "       -9.91407931e-01,  4.88358945e-01,  2.63121091e-02,  3.53342146e-01,\n",
       "       -2.65508652e-01,  3.06324959e-01,  6.37794554e-01,  8.44831705e-01,\n",
       "       -4.64367121e-01,  4.76001054e-02,  8.42857435e-02, -4.44776803e-01,\n",
       "       -6.18438065e-01, -6.41210973e-02, -1.26695395e-01,  3.15385580e-01,\n",
       "       -1.58160448e-01,  1.00128996e+00,  7.68516839e-01, -2.24973895e-02,\n",
       "        4.41031456e-01,  6.62857890e-01,  7.60835335e-02, -5.18996179e-01,\n",
       "        1.71395950e-02,  4.42842275e-01,  3.96906555e-01,  3.98113191e-01,\n",
       "       -3.53075832e-01, -2.34371182e-02,  9.49223861e-02, -6.68608351e-03,\n",
       "       -5.19271791e-01,  6.19473934e-01,  1.23899770e+00, -3.20413560e-01,\n",
       "        1.26879358e+00, -1.14056416e-01, -4.83068228e-01,  1.15950429e+00,\n",
       "       -5.37248373e-01, -3.16406459e-01,  7.43225589e-02,  2.89248645e-01,\n",
       "        3.98276508e-01,  4.57499862e-01, -2.60710925e-01,  5.71709633e-01,\n",
       "       -1.25235811e-01, -2.12135464e-02, -2.29458019e-01,  2.37815306e-01,\n",
       "        5.53947508e-01,  3.64065826e-01, -6.11326039e-01, -1.82546496e-01,\n",
       "        3.27841967e-01, -5.50046146e-01,  3.71595770e-01,  5.40783226e-01,\n",
       "        4.36416298e-01,  4.86980528e-01,  5.07264853e-01,  7.90752321e-02,\n",
       "       -5.34721673e-01, -3.26545298e-01,  7.05276608e-01,  5.92299663e-02,\n",
       "        1.05077958e+00, -6.72977030e-01,  2.66153961e-01,  6.90510333e-01,\n",
       "        2.47717112e-01, -1.12802513e-01,  4.06338185e-01,  1.73902583e+00,\n",
       "        2.62152255e-01, -2.93876439e-01, -3.61088276e-01, -1.27392876e+00,\n",
       "        4.89859700e-01,  5.45164406e-01,  6.06656492e-01, -4.12337095e-01,\n",
       "       -6.49155915e-01, -3.74258727e-01, -8.20621103e-03, -2.65670300e-01,\n",
       "       -1.05006969e+00, -1.21159971e-01, -1.26813322e-01, -2.61181921e-01,\n",
       "       -8.60716820e-01, -4.73367751e-01,  2.96182871e-01,  1.06403567e-01,\n",
       "        4.18315418e-02,  1.79089412e-01, -1.68864250e-01,  6.78414106e-02,\n",
       "       -3.40529591e-01,  5.62836587e-01,  2.13440955e-01, -7.48072982e-01,\n",
       "        4.42986727e-01,  2.05962315e-01, -1.90995082e-01, -8.57123852e-01,\n",
       "       -4.98612523e-01,  9.11414206e-01,  1.51706696e-01,  6.06753007e-02,\n",
       "        3.13859880e-02, -9.35353458e-01, -5.51654994e-01,  4.48199570e-01,\n",
       "        3.89187664e-01, -4.48384762e-01, -8.90864372e-01, -1.33716643e-01,\n",
       "       -1.21971719e-01,  8.96374062e-02,  4.73197788e-01, -1.73619553e-01,\n",
       "       -1.74233332e-01,  6.19562984e-01, -4.36696887e-01,  4.17423844e-01,\n",
       "        1.52816221e-01, -3.05978566e-01, -8.92785549e-01, -2.48361960e-01,\n",
       "        6.79455638e-01,  4.18386340e-01,  8.40618312e-01, -1.85059309e-01,\n",
       "       -8.11377943e-01,  1.03505552e-01,  2.90131178e-02, -2.42349640e-01,\n",
       "       -1.38261661e-01, -3.21392477e-01, -2.23226249e-01, -2.46769249e-01,\n",
       "        6.71776235e-02,  4.05719783e-03,  5.13760030e-01, -9.15667340e-02,\n",
       "       -3.38691543e-03, -2.59314984e-01, -2.49650016e-01,  3.07460487e-01,\n",
       "       -1.27068555e+00, -1.03399038e+00,  1.04949379e+00,  5.15443206e-01,\n",
       "        5.37699759e-01, -6.41458094e-01,  8.24197158e-02,  1.05062045e-01,\n",
       "       -4.39528674e-02, -5.44465244e-01, -8.99812460e-01,  2.22689256e-01,\n",
       "        4.83312219e-01,  3.90720814e-01, -5.97419679e-01,  4.64073196e-02,\n",
       "       -1.40537739e-01,  5.83846986e-01, -2.07215399e-01,  2.09901243e-01,\n",
       "        6.95918024e-01,  3.36631268e-01, -3.11709017e-01, -1.76820725e-01,\n",
       "       -2.61956573e-01, -4.83684421e-01,  6.43622696e-01, -5.02967358e-01,\n",
       "       -4.09226090e-01,  4.32867557e-01,  5.02589345e-01, -4.77925211e-01,\n",
       "       -3.64735037e-01, -3.79940987e-01,  1.42313719e-01,  3.69208127e-01,\n",
       "        5.07493541e-02, -1.30216414e-02,  4.07720134e-02,  7.09174156e-01,\n",
       "       -2.42942497e-01, -9.80453864e-02,  1.34562016e-01, -5.39582372e-02,\n",
       "       -1.01681411e+00, -2.62706906e-01, -1.44775271e-01,  2.26696104e-01,\n",
       "       -7.14978755e-01, -1.50910288e-01, -5.15870094e-01,  6.65029138e-03,\n",
       "        1.38898760e-01,  1.18882191e+00, -3.93813998e-01, -5.44505000e-01,\n",
       "        3.52610081e-01,  7.19145954e-01,  7.39243209e-01,  9.29905474e-02,\n",
       "        3.48079026e-01, -1.09900475e+00,  8.52841213e-02, -2.16581672e-01,\n",
       "        2.27086753e-01, -1.94936961e-01,  2.46246144e-01, -9.70711529e-01,\n",
       "       -2.01446846e-01, -1.72911227e-01, -5.85300662e-02, -1.36408865e-01,\n",
       "        2.46865973e-01,  2.50280816e-02,  4.44376349e-01, -1.98306795e-02,\n",
       "       -3.01970658e-03,  3.07300594e-02,  6.84396684e-01,  4.39548284e-01,\n",
       "       -8.51766169e-01, -3.07950109e-01,  6.91411316e-01, -2.60290712e-01,\n",
       "        6.71368718e-01, -3.24616849e-01, -2.06330970e-01,  2.41422299e-02,\n",
       "       -8.14686120e-02, -1.08796909e-01, -2.02914804e-01, -7.36877501e-01,\n",
       "        5.06197393e-01,  5.77799737e-01, -2.28224054e-01,  2.19224527e-01,\n",
       "        8.64829481e-01,  3.42028171e-01, -3.69342178e-01,  6.52160309e-03,\n",
       "       -1.09723711e+00, -4.39942777e-01, -2.85300106e-01, -2.55741358e-01,\n",
       "       -1.21292198e+00, -3.37950289e-01, -4.04231846e-01,  2.60869652e-01,\n",
       "       -1.43891454e-01, -3.28109592e-01, -1.78947315e-01,  5.13659567e-02,\n",
       "        3.52239132e-01, -4.34178472e-01, -9.76322126e-03, -2.17980191e-01,\n",
       "       -2.64736116e-01, -1.46877438e-01, -1.11648858e+00,  4.76819634e-01,\n",
       "        6.04727685e-01,  3.35001111e-01,  3.91297787e-01,  1.05548859e+00,\n",
       "        6.76586568e-01,  9.74343047e-02,  9.99029517e-01,  4.48856682e-01,\n",
       "        5.28638780e-01,  3.57035577e-01,  5.15456438e-01, -9.17389914e-02,\n",
       "        2.99743563e-01,  3.09677776e-02, -1.64316282e-01, -3.16451818e-01,\n",
       "        6.15149438e-01, -2.82734811e-01,  4.16567177e-01,  4.15705502e-01,\n",
       "        9.33886915e-02, -6.26557827e-01,  6.84033453e-01, -6.34713948e-01,\n",
       "       -2.91729510e-01,  3.96063030e-01,  3.61634754e-02,  4.87941742e-01,\n",
       "       -4.40445244e-01, -2.90709347e-01,  7.66516566e-01,  7.26568937e-01,\n",
       "        2.74393596e-02, -1.44232646e-01,  8.83555859e-02, -1.83973938e-01,\n",
       "       -1.49343327e-01, -2.59530813e-01,  9.15988445e-01,  5.25626838e-01,\n",
       "        3.78000915e-01, -2.58428514e-01, -2.83891167e-02,  1.19669342e+00,\n",
       "        1.19365610e-01,  7.14534402e-01,  6.90945685e-02, -6.46565020e-01,\n",
       "       -1.13931082e-01, -1.27705500e-01, -8.42572212e-01, -3.49861324e-01,\n",
       "        1.57597382e-03, -1.75667018e-01,  7.82854974e-01,  4.76683944e-01,\n",
       "        1.46457702e-01, -3.38904709e-02,  3.52286667e-01, -8.17921851e-03,\n",
       "        1.61622092e-01,  6.43778980e-01, -6.49339855e-01, -3.66825730e-01,\n",
       "       -1.20570138e-01,  3.63596678e-01, -1.60772070e-01, -2.45653808e-01,\n",
       "       -1.39651388e-01, -4.18021768e-01, -5.58367014e-01,  1.77140608e-01,\n",
       "       -7.01394141e-01,  6.51077330e-01,  5.52822530e-01, -3.27879608e-01,\n",
       "        4.65555862e-02, -2.12612361e-01, -1.32958442e-01,  9.13356990e-03,\n",
       "       -7.40401089e-01,  1.12905644e-01,  8.00992489e-01, -2.05126196e-01,\n",
       "        2.89042443e-01,  7.10492969e-01,  2.26464927e-01, -1.78762928e-01,\n",
       "       -2.87530005e-01, -3.53256673e-01,  2.48305291e-01, -8.65377009e-01,\n",
       "        2.27456778e-01,  1.32479787e-01,  2.83608228e-01, -1.01818097e+00,\n",
       "        1.36174262e-01, -4.07526940e-01, -1.19046366e+00, -6.00569665e-01,\n",
       "       -2.04110757e-01, -3.04014474e-01, -9.80412543e-01,  4.58824448e-02,\n",
       "       -1.39935613e+00, -1.50567210e+00, -1.84518024e-01, -7.09174275e-02,\n",
       "        4.06739712e-02, -2.85628736e-02,  7.23898113e-01,  4.85761076e-01,\n",
       "        5.05742848e-01, -6.29522264e-01,  1.38830543e-01, -4.17289674e-01,\n",
       "        7.43021309e-01, -1.15505347e-04, -3.17714304e-01, -1.40851244e-01,\n",
       "        4.55894172e-02,  9.88093555e-01,  2.11995527e-01,  4.35665458e-01,\n",
       "        8.88266116e-02, -2.23910660e-01, -3.98780629e-02,  9.94353667e-02,\n",
       "        9.18952882e-01,  1.12941790e+00, -8.77247334e-01, -3.82805586e-01,\n",
       "       -2.67671436e-01, -8.16215932e-01,  4.08783890e-02,  5.57425842e-02,\n",
       "        1.94419362e-02, -3.59093547e-01,  3.49751621e-01,  2.43480951e-02,\n",
       "       -9.75307882e-01, -2.78323531e-01,  5.26512146e-01,  3.74670476e-01,\n",
       "       -1.78480089e-01, -1.48808330e-01, -1.67656675e-01, -5.84338605e-02,\n",
       "        8.56834412e-01,  9.08757567e-01,  8.57590079e-01, -3.40181142e-02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_to_vec(['And mine too, please!'], m, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# cosine simularity и ембедить предложения\n",
    "def embeding_cos_similarity(words1, words2):\n",
    "    words1 = document_to_vec(words1, m, dim=768)\n",
    "    words2 = document_to_vec(words2, m, dim=768)\n",
    "    if not len(words1) or not len(words2):\n",
    "        return 0.0\n",
    "    return cosine_similarity([words1], [words2])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2GwyRrMPAzS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 5770\n",
      "Ref: киев должен прямо ответить, намерен ли он выполнять dторое минское соглашение — с таким заявлением обратился президент россии владимир путин к своему украинскому коллеге владимиру зеленскому в ходе телефонного разговора. риторика российского лидера обусловлена рядом недавних высказываний украинских властей относительно своего видения урегулирования конфликта в донбассе.\n",
      "Hyp: в ходе беседы президент россии особо отметил важность полной и безусловной реализации также и положений, принятых лидерами россии, украины, германии и франции на саммите в «нормандском формате», прошедшем в париже 9 декабря прошлого года. «эти вопросы должны решаться в режиме прямого политического диалога между киевом и донбассом  киевская власть не сможет восстановить отношения с донбассом, пока не начнет разговаривать с жителями донбасса», — подчеркнул грызлов.\n",
      "BLEU:  0.27748426882615645\n",
      "ROUGE:  {'rouge-1': {'f': 0.16185144539360458, 'p': 0.1349236513608401, 'r': 0.21801411088292716}, 'rouge-2': {'f': 0.03746646648852568, 'p': 0.030486798772845308, 'r': 0.052870599268185034}, 'rouge-l': {'f': 0.12861367363664242, 'p': 0.1198642374717009, 'r': 0.19368574568061167}}\n",
      "Count: 5770\n",
      "Ref: киев должен прямо ответить, намерен ли он выполнять dторое минское соглашение — с таким заявлением обратился президент россии владимир путин к своему украинскому коллеге владимиру зеленскому в ходе телефонного разговора. риторика российского лидера обусловлена рядом недавних высказываний украинских властей относительно своего видения урегулирования конфликта в донбассе.\n",
      "Hyp: «эти вопросы должны решаться в режиме прямого политического диалога между киевом и донбассом  киевская власть не сможет восстановить отношения с донбассом, пока не начнет разговаривать с жителями донбасса», — подчеркнул грызлов. «я обратился к премьер-министру с тем, чтобы были закрыты незаконные офисы днр и лнр, которые открылись в нескольких городах италии», — пояснил украинский лидер в комментариях для сми после встречи.\n",
      "BLEU:  0.27672176170202756\n",
      "ROUGE:  {'rouge-1': {'f': 0.16864078441328575, 'p': 0.13786184889275063, 'r': 0.23373729011783886}, 'rouge-2': {'f': 0.0419841367084186, 'p': 0.03355665221693976, 'r': 0.06098046117104547}, 'rouge-l': {'f': 0.13240923375579836, 'p': 0.12248866669027247, 'r': 0.20767041429649163}}\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "def unique_words_similarity(words1, words2):\n",
    "    '''\n",
    "    Функция подсчёта близости предложений на основе пересечения слов\n",
    "    ''' \n",
    "    words1 = set(words1)\n",
    "    words2 = set(words2)\n",
    "    if not len(words1) or not len(words2):\n",
    "        return 0.0\n",
    "    return len(words1.intersection(words2))/(np.log10(len(words1)) + np.log10(len(words2)))\n",
    "\n",
    "def gen_text_rank_summary(text, calc_similarity=embeding_cos_similarity, summary_part=0.1, lower=True, morph=None):\n",
    "    '''\n",
    "    Составление summary с помощью TextRank\n",
    "    '''\n",
    "    # Разбиваем текст на предложения\n",
    "    sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    # Токенизируем предложения\n",
    "    sentences_words = [[token.text.lower() if lower else token.text for token in razdel.tokenize(sentence)] for sentence in sentences]\n",
    "\n",
    "    # При необходимости лемматизируем слова\n",
    "    if morph is not None:\n",
    "        sentences_words = [[morph.parse(word)[0].normal_form for word in words] for words in sentences_words]  \n",
    "    \n",
    "    \n",
    "    # Для каждой пары предложений считаем близость\n",
    "    pairs = combinations(range(n_sentences), 2)\n",
    "    scores = [(i, j, calc_similarity(sentences_words[i], sentences_words[j])) for i, j in pairs]\n",
    "\n",
    "    # Строим граф с рёбрами, равными близости между предложениями\n",
    "    g = nx.Graph()\n",
    "    g.add_weighted_edges_from(scores)\n",
    "\n",
    "    # Считаем PageRank\n",
    "    pr = nx.pagerank(g)\n",
    "    result = [(i, pr[i], s) for i, s in enumerate(sentences) if i in pr]\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Выбираем топ предложений\n",
    "    n_summary_sentences = max(int(n_sentences * summary_part), 1)\n",
    "    result = result[:n_summary_sentences]\n",
    "\n",
    "    # Восстанавливаем оригинальный их порядок\n",
    "    result.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Восстанавливаем текст выжимки\n",
    "    predicted_summary = \" \".join([sentence for i, proba, sentence in result])\n",
    "    predicted_summary = predicted_summary.lower() if lower else predicted_summary\n",
    "    return predicted_summary\n",
    "\n",
    "def calc_text_rank_score(records, calc_similarity=unique_words_similarity, summary_part=0.1, lower=True, nrows=5770, morph=None):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for text, summary in records[['text', 'summary']].values[:nrows]:\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        predicted_summary = gen_text_rank_summary(text, calc_similarity, summary_part, lower, morph=morph)\n",
    "        text = text if not lower else text.lower()\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "calc_text_rank_score(test_records)\n",
    "calc_text_rank_score(test_records, morph=morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdTrfxycB7cd"
   },
   "source": [
    "## 2 Задание: Extractive RNN (порог: 0.3 BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q7DeHDYFSjX"
   },
   "source": [
    "Второй метод, который вам предлагается улучшить – поиск предложений для summary с помощью RNN. В рассмотренной методе мы использовали LSTM для генерации sentence embedding. Попробуйте использовать другие архитектуры: CNN, Transformer; или добавьте предобученные модели, как и в первом задании.\n",
    "\n",
    "P.S. Тут предполагается, что придется изменять много кода в ячееках (например, поменять токенизацию). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dZamxigdEc-"
   },
   "source": [
    "### Модель\n",
    "\n",
    "Картинка для привлечения внимания:\n",
    "\n",
    "![img](https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_398421%2Fimages%2Farchitecture.png)\n",
    "\n",
    "Статья с оригинальным методом:\n",
    "https://arxiv.org/pdf/1611.04230.pdf\n",
    "\n",
    "Список вдохновения: \n",
    "- https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b Пример того, как можно применять CNN в текстовых задачах\n",
    "- https://arxiv.org/pdf/1808.08745.pdf Очень крутой метод генерации summary без Transformers\n",
    "- https://towardsdatascience.com/super-easy-way-to-get-sentence-embedding-using-fasttext-in-python-a70f34ac5b7c – простой метод генерации sentence embedding\n",
    "- https://towardsdatascience.com/fse-2b1ffa791cf9 – Необычный метод генерации sentence embedding\n",
    "- https://github.com/UKPLab/sentence-transformers – BERT предобученный для sentence embedding\n",
    "\n",
    "P.S. Выше написанные ссылки нужны только для разогрева вашей фантазии, можно воспользоваться ими, а можно придумать свой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOH4ZbLkg_sM"
   },
   "source": [
    "Комментарий к заданию:\n",
    "Если посмотреть на архитектуру ~~почти~~ SummaRuNNer, то в ней есть два главных элемента: первая часть, которая читает предложения и возвращает векторы на каждое предложение, и вторая, которая выбирает предложения для суммаризации. Вторую часть мы не трогаем, а первую меняем. На что меняем – как вы решите. Главное: она должна иметь хорошее качество и встроиться в текущую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sxsc0Orf8hGq"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
    "    '''\n",
    "    Жадное построение oracle summary\n",
    "    '''\n",
    "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
    "    # Делим текст на предложения\n",
    "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "    n_sentences = len(sentences)\n",
    "    oracle_summary_sentences = set()\n",
    "    score = -1.0\n",
    "    summaries = []\n",
    "    for _ in range(min(n_sentences, 2)):\n",
    "        for i in range(n_sentences):\n",
    "            if i in oracle_summary_sentences:\n",
    "                continue\n",
    "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
    "            # Добавляем какое-то предложения к уже существующему summary\n",
    "            current_summary_sentences.add(i)\n",
    "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
    "            # Считаем метрики\n",
    "            current_score = calc_score(current_summary, gold_summary)\n",
    "            summaries.append((current_score, current_summary_sentences))\n",
    "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
    "        # Иначе на этом заканчиваем\n",
    "        best_summary_score, best_summary_sentences = max(summaries)\n",
    "        if best_summary_score <= score:\n",
    "            break\n",
    "        oracle_summary_sentences = best_summary_sentences\n",
    "        score = best_summary_score\n",
    "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
    "    return oracle_summary, oracle_summary_sentences\n",
    "\n",
    "def calc_single_score(pred_summary, gold_summary, rouge):\n",
    "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7T_ak-KDB8rp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d92cf2e1ec0456284f44b8bacb12af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count: 1000\n",
      "Ref: президенты россии и турции владимир путин и реджеп тайип эрдоган обсудили вопрос признания сша иерусалима столицей израиля по телефону. российский лидер поддержал позицию главы турции и подчеркнул, что решение сша способствует росту напряженности в регионе.\n",
      "Hyp: стороны также отметили недопустимость дальнейшей эскалации напряженности в регионе и подтвердили приверженность россии и турции достижению справедливого и жизнеспособного решения ближневосточного кризиса на основе соответствующих резолюций совета безопасности и генеральной ассамблеи оон. пресс-секретарь президента россии дмитрий песков заявил, что решение президента сша дональда трампа о признании иерусалима столицей израиля не способствует ближневосточному урегулированию.\n",
      "BLEU:  0.415488069082347\n",
      "ROUGE:  {'rouge-1': {'f': 0.31210835269088893, 'p': 0.37392704527998943, 'r': 0.28535927895414975}, 'rouge-2': {'f': 0.1666899310777806, 'p': 0.20579618805246494, 'r': 0.15013773493648735}, 'rouge-l': {'f': 0.26606501670575183, 'p': 0.3454095980816058, 'r': 0.2619396927658744}}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def calc_oracle_score(records, nrows=1000, lower=True):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    rouge = Rouge()\n",
    "  \n",
    "    for text, summary in tqdm(records[['text', 'summary']].values[:nrows]):\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "calc_oracle_score(train_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('train_records.pickle', 'wb') as f:\n",
    "#     pickle.dump(train_records, f)\n",
    "\n",
    "# # with open('train_records.pickle', 'rb') as f:\n",
    "# # data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWgjewfWrbJZ"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код загрузки токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qIRKm4TCHzN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n",
    "    temp_file_name = \"temp.txt\"\n",
    "    with open(temp_file_name, \"w\") as temp:\n",
    "        for text, summary in records[['text', 'summary']].values:\n",
    "            if lower:\n",
    "                summary = summary.lower()\n",
    "                text = text.lower()\n",
    "            if not text or not summary:\n",
    "                continue\n",
    "            temp.write(text + \"\\n\")\n",
    "            temp.write(summary + \"\\n\")\n",
    "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n",
    "\n",
    "train_bpe(train_records, \"BPE_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAkZ2f5LhWwE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁окт', 'ябрь', '▁бога', 'т', '▁на', '▁изменения']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import youtokentome as yttm\n",
    "\n",
    "bpe_processor = yttm.BPE('BPE_model.bin')\n",
    "bpe_processor.encode([\"октябрь богат на изменения\"], output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOkUL_YIGp-S"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhQYN1beiVEC"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, bpe_processor):\n",
    "        self.index2word = bpe_processor.vocab()\n",
    "        self.word2index = {w: i for i, w in enumerate(self.index2word)}\n",
    "        self.word2count = Counter()\n",
    "\n",
    "    def get_pad(self):\n",
    "        return self.word2index[\"<PAD>\"]\n",
    "\n",
    "    def get_sos(self):\n",
    "        return self.word2index[\"<SOS>\"]\n",
    "\n",
    "    def get_eos(self):\n",
    "        return self.word2index[\"<EOS>\"]\n",
    "\n",
    "    def get_unk(self):\n",
    "        return self.word2index[\"<UNK>\"]\n",
    "    \n",
    "    def has_word(self, word) -> bool:\n",
    "        return word in self.word2index\n",
    "\n",
    "    def get_index(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        return self.get_unk()\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def is_empty(self):\n",
    "        empty_size = 4\n",
    "        return self.size() <= empty_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.word2count = Counter()\n",
    "        self.index2word = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.word2index = {word: index for index, word in enumerate(self.index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qvZtNcOifAn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = Vocabulary(bpe_processor)\n",
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jdb-39jO-72q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20413748a5554d90bb0ca5540d6a2abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4096.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f36f90ae484f9ebab72e1d7705c4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110cffdc9938468ea4aed1a0eb02b458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import razdel\n",
    "\n",
    "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
    "    rouge = Rouge()\n",
    "    sentences_ = []\n",
    "    oracle_sentences_ = []\n",
    "    oracle_summary_ = []\n",
    "    records = records.iloc[:nrows].copy()\n",
    "\n",
    "    for text, summary in tqdm(records[['text', 'summary']].values):\n",
    "        summary = summary.lower() if lower else summary\n",
    "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
    "                                                                         lower=lower, max_sentences=max_sentences)\n",
    "        sentences_ += [sentences]\n",
    "        oracle_sentences_ += [list(sentences_indicies)]\n",
    "        oracle_summary_ += [oracle_summary]\n",
    "    records['sentences'] = sentences_\n",
    "    records['oracle_sentences'] = oracle_sentences_\n",
    "    records['oracle_summary'] = oracle_summary_\n",
    "    return records\n",
    "\n",
    "ext_train_records = add_oracle_summary_to_records(train_records, nrows=4096)\n",
    "ext_val_records = add_oracle_summary_to_records(val_records, nrows=256)\n",
    "ext_test_records = add_oracle_summary_to_records(test_records, nrows=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # with open('ext_train_records.pickle', 'wb') as f:\n",
    "# #     pickle.dump(ext_train_records, f)\n",
    "\n",
    "# # with open('ext_val_records.pickle', 'wb') as f:\n",
    "# #     pickle.dump(ext_val_records, f)\n",
    "\n",
    "# # with open('ext_test_records.pickle', 'wb') as f:\n",
    "# #     pickle.dump(ext_test_records, f)\n",
    "\n",
    "    \n",
    "# with open('ext_train_records.pickle', 'rb') as f:\n",
    "#     ext_train_records = pickle.load(f)\n",
    "\n",
    "# with open('ext_val_records.pickle', 'rb') as f:\n",
    "#     ext_val_records = pickle.load(f)\n",
    "\n",
    "# with open('ext_test_records.pickle', 'rb') as f:\n",
    "#     ext_test_records = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                 https://www.gazeta.ru/politics/2011/07/06_a_36...\n",
       "text                Вопрос об округе, от которого Валентина Матвие...\n",
       "title                                   Матвиенко сдался первый совет\n",
       "summary             Валентина Матвиенко может стать муниципальным ...\n",
       "date                                              2011-07-06 15:00:04\n",
       "sentences           [вопрос об округе, от которого валентина матви...\n",
       "oracle_sentences                                               [0, 9]\n",
       "oracle_summary      вопрос об округе, от которого валентина матвие...\n",
       "Name: 41368, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_train_records.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('russian'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), '', text)\n",
    "    text = re.sub('[^А-Яа-я\\. ]', '', text)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for stopWord in stopWords:\n",
    "        # text = text.replace(stopWord, '')\n",
    "        text = re.sub('^{}$'.format(stopWord), '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import razdel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class ExtDataset(data.Dataset):\n",
    "    def __init__(self, records, vocabulary, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
    "        self.records = records\n",
    "        self.num_samples = records.shape[0]\n",
    "        self.bpe_processor = bpe_processor\n",
    "        self.lower = lower\n",
    "        self.rouge = Rouge()\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.records.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_record = self.records.iloc[idx]\n",
    "        inputs = list(map(lambda x: text_prepare(x), cur_record['sentences']))\n",
    "        inputs = list(map(lambda x: [i.text for i in razdel.tokenize(x[:self.max_sentence_length])], inputs))\n",
    "        inputs = list(map(lambda x: [morph.parse(word)[0].normal_form for word in x] , inputs))\n",
    "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
    "        return {'inputs': inputs, 'outputs': outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExtDataset(ext_train_records, vocabulary)\n",
    "test_dataset = ExtDataset(ext_test_records, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('ext_train_records.pickle', 'wb') as f:\n",
    "#     pickle.dump(ext_train_records, f)\n",
    "\n",
    "# with open('ext_val_records.pickle', 'wb') as f:\n",
    "#     pickle.dump(ext_val_records, f)\n",
    "\n",
    "# with open('ext_test_records.pickle', 'wb') as f:\n",
    "#     pickle.dump(ext_test_records, f)\n",
    "\n",
    "    \n",
    "# # with open('train_records.pickle', 'rb') as f:\n",
    "# # data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [['вопрос',\n",
       "   'о',\n",
       "   'округа',\n",
       "   'от',\n",
       "   'который',\n",
       "   'валентин',\n",
       "   'матвиенко',\n",
       "   'плата'],\n",
       "  ['полномочие', 'утратить', 'муниципальный', 'совет', 'в', 'посёлок', 'а'],\n",
       "  ['в', 'заявление', 'баранников', 'написать', 'мой', 'решение', 'добро'],\n",
       "  ['оно', 'вызвать', 'желание', 'предоставить', 'возможность', 'и', 'с'],\n",
       "  ['ирина', 'баранников', 'в', 'девичество', 'волох', 'год', 'она', 'чле'],\n",
       "  ['в', 'муниципалитет', 'она', 'возглавлять', 'комиссия', 'по', 'борьба'],\n",
       "  ['баллотироваться', 'в', 'депутат', 'от', 'справедливый', 'россия'],\n",
       "  ['она', 'отец', 'тоже', 'единоросс', 'главный', 'федеральный', 'инспечь'],\n",
       "  ['представлять', 'он', 'в', 'год', 'член', 'правительство', 'санкт'],\n",
       "  ['теперь', 'в', 'округа', 'быть', 'объявить', 'досрочный', 'выбор', 'год'],\n",
       "  ['у', 'мы', 'быть', 'депутат', 'вместо', 'положить', 'считать'],\n",
       "  ['после', 'тот', 'как', 'ирина', 'баранников', 'написать', 'заявить'],\n",
       "  ['по', 'закон', 'совет', 'потерять', 'легитимность'],\n",
       "  ['в', 'избирательный', 'комиссия', 'уже', 'направить', 'заявление'],\n",
       "  ['дата', 'выбор', 'быть', 'объявить', 'в', 'течение', 'день'],\n",
       "  ['пресссекретарить', 'петербургский', 'губернатор', 'евгений'],\n",
       "  ['многие', 'муниципальный', 'образование', 'заявлять', 'о', 'готовна'],\n",
       "  ['так', 'что', 'выбор', 'у', 'губернатор', 'есть'],\n",
       "  ['сейчас', 'идти', 'консультация', 'сказать', 'пресссекретарить', 'год'],\n",
       "  ['глава', 'комиссия', 'по', 'законодательство', 'закса', 'единоросс'],\n",
       "  ['по',\n",
       "   'он',\n",
       "   'информация',\n",
       "   'матвиенко',\n",
       "   'объявить',\n",
       "   'о',\n",
       "   'участие',\n",
       "   'в',\n",
       "   'в'],\n",
       "  ['как', 'рассказать', 'газета', 'председатель', 'совет', 'муниципал'],\n",
       "  ['там', 'и', 'представитель', 'среднее', 'класс', 'и', 'человек', 'скромно'],\n",
       "  ['в', 'посёлок', 'идти', 'строительство', 'он', 'динамично', 'развивай'],\n",
       "  ['единственный', 'отличие', 'александровский', 'от', 'другой', 'кан'],\n",
       "  ['по', 'статус', 'для', 'губернатор', 'переходящий', 'в', 'совет', 'фе'],\n",
       "  ['другой', 'одномандатный', 'муниципалитет', 'петербург', 'эт'],\n",
       "  ['последний', 'район', 'источник', 'в', 'окружение', 'матвиенко', 'уж'],\n",
       "  ['однако', 'глава', 'муниципалитет', 'светлана', 'зряхов', 'отказ'],\n",
       "  ['в', 'среда', 'депутатыединоросс', 'и', 'один', 'независимый', 'деп']],\n",
       " 'outputs': [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f989f3ad28de4098b66b2f593a6f894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4096.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff53445753db43eea257945d120357f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fse_train_dataset = []\n",
    "fse_test_dataset = []\n",
    "fse_train_dataset = [j for i in tqdm(train_dataset) for j in i['inputs']]\n",
    "fse_test_dataset = [j for i in tqdm(test_dataset) for j in i['inputs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # with open('fse_train_dataset.pickle', 'wb') as f:\n",
    "# #     pickle.dump(fse_train_dataset, f)\n",
    "\n",
    "# # with open('fse_test_dataset.pickle', 'wb') as f:\n",
    "# #     pickle.dump(fse_test_dataset, f)\n",
    "    \n",
    "# with open('fse_train_dataset.pickle', 'rb') as f:\n",
    "#     fse_train_dataset = pickle.load(f)\n",
    "\n",
    "# with open('fse_test_dataset.pickle', 'rb') as f:\n",
    "#     fse_test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116810"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fse_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['в', 'заявление', 'баранников', 'написать', 'мой', 'решение', 'добро'], 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fse import IndexedList\n",
    "indexed_tokenized_sentences_train = IndexedList(fse_train_dataset)\n",
    "indexed_tokenized_sentences_test = IndexedList(fse_test_dataset)\n",
    "indexed_tokenized_sentences_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    fname='ft_native_300_ru_wiki_lenta_lower_case.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7411, 53082)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fse.models import Average\n",
    "fse_model = Average(wv_embeddings, workers=2)\n",
    "fse_model.train(indexed_tokenized_sentences_train)\n",
    "fse_model.train(indexed_tokenized_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import razdel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class ExtDataset(data.Dataset):\n",
    "    def __init__(self, records, vocabulary, fse_model=fse_model, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
    "        self.records = records\n",
    "        self.num_samples = records.shape[0]\n",
    "        self.bpe_processor = bpe_processor\n",
    "        self.lower = lower\n",
    "        self.rouge = Rouge()\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.device = device\n",
    "        self.model = fse_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.records.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_record = self.records.iloc[idx]\n",
    "        inputs = list(map(lambda x: text_prepare(x), cur_record['sentences']))\n",
    "        inputs = list(map(lambda x: [i.text for i in razdel.tokenize(x[:self.max_sentence_length])], inputs))\n",
    "        inputs = list(map(lambda x: [morph.parse(word)[0].normal_form for word in x] , inputs))\n",
    "        inputs = self.model.infer(IndexedList(inputs))\n",
    "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
    "        return {'inputs': inputs, 'outputs': outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExtDataset(ext_train_records, vocabulary, fse_model)\n",
    "test_dataset = ExtDataset(ext_test_records, vocabulary, fse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0]['inputs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlXXc8qUHC5m"
   },
   "source": [
    "## (!)\n",
    "Если надо, поменяйте код генератора датасета и батчевалки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvARjudojEDD"
   },
   "outputs": [],
   "source": [
    "# Это батчевалка\n",
    "def collate_fn(records):\n",
    "    max_length = max(len(sentence) for record in records for sentence in record['inputs'])\n",
    "    max_sentences = max(len(record['outputs']) for record in records)\n",
    "\n",
    "    new_inputs = torch.zeros((len(records), max_sentences, max_length))\n",
    "    new_outputs = torch.zeros((len(records), max_sentences))\n",
    "    for i, record in enumerate(records):\n",
    "        for j, sentence in enumerate(record['inputs']):\n",
    "            new_inputs[i, j, :len(sentence)] += np.array(sentence)\n",
    "        new_outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
    "    return {'features': new_inputs.type(torch.FloatTensor), 'targets': new_outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWlf7XdheJUN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "class SentenceTaggerRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 token_embedding_dim=300,\n",
    "                 sentence_encoder_hidden_size=300,\n",
    "                 hidden_size=128,\n",
    "                 bidirectional=True,\n",
    "                 sentence_encoder_n_layers=2,\n",
    "                 sentence_encoder_dropout=0.3,\n",
    "                 sentence_encoder_bidirectional=True,\n",
    "                 n_layers=1,\n",
    "                 dropout=0.3):\n",
    "        super(SentenceTaggerRNN, self).__init__()\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n",
    "                           bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
    "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.classify = nn.Sigmoid()\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        batch_size = inputs.size(0)\n",
    "        sentences_count = inputs.size(1)\n",
    "        tokens_count = inputs.size(2)\n",
    "        inputs = inputs.reshape(-1, tokens_count)\n",
    "#         embedded_sentences = self.sentence_encoder(inputs)\n",
    "        embedded_sentences = inputs\n",
    "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
    "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
    "        content = self.content_linear_layer(outputs).squeeze(2)\n",
    "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2)\n",
    "        out = self.classify(content + salience)\n",
    "\n",
    "#         print(F.softmax(out))\n",
    "        return F.softmax(out)\n",
    "\n",
    "model = SentenceTaggerRNN(vocabulary.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4q2Gb6ODHHB_"
   },
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVDW8raJeQxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.228]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   3% 1/32 [00:05<03:03,  5.93s/it, loss=0.228]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1/10 * Epoch (train):   3% 1/32 [00:11<03:03,  5.93s/it, loss=0.231]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   6% 2/32 [00:11<02:58,  5.96s/it, loss=0.231]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   6% 2/32 [00:17<02:58,  5.96s/it, loss=0.227]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   9% 3/32 [00:17<02:52,  5.94s/it, loss=0.227]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):   9% 3/32 [00:23<02:52,  5.94s/it, loss=0.226]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  12% 4/32 [00:23<02:45,  5.90s/it, loss=0.226]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  12% 4/32 [00:29<02:45,  5.90s/it, loss=0.227]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  16% 5/32 [00:29<02:39,  5.89s/it, loss=0.227]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  16% 5/32 [00:35<02:39,  5.89s/it, loss=0.225]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  19% 6/32 [00:35<02:31,  5.84s/it, loss=0.225]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  19% 6/32 [00:41<02:31,  5.84s/it, loss=0.223]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  22% 7/32 [00:41<02:25,  5.82s/it, loss=0.223]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.82s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.80s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.80s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.76s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  28% 9/32 [00:58<02:12,  5.76s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  31% 10/32 [00:58<02:06,  5.75s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  31% 10/32 [01:03<02:06,  5.75s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  34% 11/32 [01:03<02:01,  5.76s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  34% 11/32 [01:09<02:01,  5.76s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  38% 12/32 [01:09<01:54,  5.73s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  38% 12/32 [01:15<01:54,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  41% 13/32 [01:15<01:49,  5.75s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  41% 13/32 [01:21<01:49,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  44% 14/32 [01:21<01:43,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.75s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.72s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.70s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.70s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  53% 17/32 [01:38<01:25,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  53% 17/32 [01:44<01:25,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  56% 18/32 [01:44<01:20,  5.74s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.74s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  59% 19/32 [01:49<01:14,  5.74s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  59% 19/32 [01:55<01:14,  5.74s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.68s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.68s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  66% 21/32 [02:01<01:02,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.71s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  69% 22/32 [02:06<00:57,  5.70s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.70s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.69s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.69s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.66s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  75% 24/32 [02:23<00:45,  5.66s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  78% 25/32 [02:23<00:39,  5.69s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  78% 25/32 [02:29<00:39,  5.69s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.72s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.72s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.72s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  84% 27/32 [02:40<00:28,  5.72s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  88% 28/32 [02:40<00:22,  5.69s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  88% 28/32 [02:46<00:22,  5.69s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  91% 29/32 [02:46<00:17,  5.68s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  91% 29/32 [02:52<00:17,  5.68s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  94% 30/32 [02:52<00:11,  5.66s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.66s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.71s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train):  97% 31/32 [03:03<00:05,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.65s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.74s/it, loss=0.214]\n",
      "\n",
      "\n",
      "1/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.92s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.92s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.90s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.90s/it, loss=0.206]\n",
      "\n",
      "\n",
      "1/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.91s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.91s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.89s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "1/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.88s/it, loss=0.211]\n",
      "[2020-05-27 21:01:23,972] \n",
      "1/10 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): loss=0.2145\n",
      "1/10 * Epoch 1 (valid): loss=0.2094\n",
      "1/10 * Epoch 1 (test): loss=0.2141\n",
      "[2020-05-27 21:01:23,972] \n",
      "1/10 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): loss=0.2145\n",
      "1/10 * Epoch 1 (valid): loss=0.2094\n",
      "1/10 * Epoch 1 (test): loss=0.2141\n",
      "[2020-05-27 21:01:23,972] \n",
      "1/10 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): loss=0.2145\n",
      "1/10 * Epoch 1 (valid): loss=0.2094\n",
      "1/10 * Epoch 1 (test): loss=0.2141\n",
      "\n",
      "\n",
      "2/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   3% 1/32 [00:05<03:01,  5.85s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   3% 1/32 [00:11<03:01,  5.85s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   6% 2/32 [00:11<02:54,  5.82s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   6% 2/32 [00:17<02:54,  5.82s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   9% 3/32 [00:17<02:48,  5.80s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):   9% 3/32 [00:23<02:48,  5.80s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  12% 4/32 [00:23<02:41,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  12% 4/32 [00:28<02:41,  5.78s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  16% 5/32 [00:28<02:35,  5.77s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  16% 5/32 [00:34<02:35,  5.77s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  22% 7/32 [00:40<02:25,  5.84s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.84s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  25% 8/32 [00:46<02:20,  5.86s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  25% 8/32 [00:52<02:20,  5.86s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  28% 9/32 [00:52<02:15,  5.87s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  28% 9/32 [00:58<02:15,  5.87s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  31% 10/32 [00:58<02:09,  5.87s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  31% 10/32 [01:03<02:09,  5.87s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  34% 11/32 [01:03<02:02,  5.82s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  34% 11/32 [01:09<02:02,  5.82s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  38% 12/32 [01:09<01:55,  5.78s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  38% 12/32 [01:15<01:55,  5.78s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 * Epoch (train):  41% 13/32 [01:15<01:49,  5.78s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  41% 13/32 [01:21<01:49,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  44% 14/32 [01:21<01:44,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  44% 14/32 [01:26<01:44,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.75s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.75s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.73s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  53% 17/32 [01:44<01:26,  5.76s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  56% 18/32 [01:44<01:20,  5.77s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  56% 18/32 [01:50<01:20,  5.77s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  59% 19/32 [01:50<01:15,  5.78s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  59% 19/32 [01:55<01:15,  5.78s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.69s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.69s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  66% 21/32 [02:01<01:02,  5.71s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.71s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  69% 22/32 [02:06<00:57,  5.71s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.71s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.71s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.71s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.68s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  75% 24/32 [02:24<00:45,  5.68s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  78% 25/32 [02:24<00:39,  5.71s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  78% 25/32 [02:29<00:39,  5.71s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.74s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  88% 28/32 [02:41<00:22,  5.69s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  88% 28/32 [02:46<00:22,  5.69s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  91% 29/32 [02:46<00:17,  5.69s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  91% 29/32 [02:52<00:17,  5.69s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  94% 30/32 [02:52<00:11,  5.66s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.66s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.71s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train):  97% 31/32 [03:03<00:05,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.65s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.74s/it, loss=0.214]\n",
      "\n",
      "\n",
      "2/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  6.00s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  6.00s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.95s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\n",
      "\n",
      "\n",
      "2/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.84s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.84s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.83s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "2/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.83s/it, loss=0.211]\n",
      "[2020-05-27 21:04:51,353] \n",
      "2/10 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): loss=0.2107\n",
      "2/10 * Epoch 2 (valid): loss=0.2093\n",
      "2/10 * Epoch 2 (test): loss=0.2141\n",
      "[2020-05-27 21:04:51,353] \n",
      "2/10 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): loss=0.2107\n",
      "2/10 * Epoch 2 (valid): loss=0.2093\n",
      "2/10 * Epoch 2 (test): loss=0.2141\n",
      "[2020-05-27 21:04:51,353] \n",
      "2/10 * Epoch 2 (_base): lr=0.0010 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): loss=0.2107\n",
      "2/10 * Epoch 2 (valid): loss=0.2093\n",
      "2/10 * Epoch 2 (test): loss=0.2141\n",
      "\n",
      "\n",
      "3/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   3% 1/32 [00:05<02:59,  5.80s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   3% 1/32 [00:11<02:59,  5.80s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   6% 2/32 [00:11<02:53,  5.79s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   6% 2/32 [00:17<02:53,  5.79s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   9% 3/32 [00:17<02:47,  5.78s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):   9% 3/32 [00:23<02:47,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  12% 4/32 [00:23<02:41,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  12% 4/32 [00:28<02:41,  5.77s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  16% 5/32 [00:28<02:35,  5.77s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  16% 5/32 [00:34<02:35,  5.77s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.80s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  22% 7/32 [00:40<02:24,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  22% 7/32 [00:46<02:24,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  25% 8/32 [00:46<02:18,  5.78s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  25% 8/32 [00:51<02:18,  5.78s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  28% 9/32 [00:51<02:12,  5.76s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  28% 9/32 [00:57<02:12,  5.76s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  31% 10/32 [00:57<02:06,  5.76s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  31% 10/32 [01:03<02:06,  5.76s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  34% 11/32 [01:03<02:00,  5.74s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  34% 11/32 [01:09<02:00,  5.74s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  38% 12/32 [01:09<01:54,  5.72s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  38% 12/32 [01:14<01:54,  5.72s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  41% 13/32 [01:14<01:48,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  41% 13/32 [01:20<01:48,  5.74s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  44% 14/32 [01:20<01:43,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.71s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  47% 15/32 [01:31<01:37,  5.71s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  50% 16/32 [01:31<01:30,  5.68s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  50% 16/32 [01:37<01:30,  5.68s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  53% 17/32 [01:37<01:25,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  53% 17/32 [01:43<01:25,  5.72s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  56% 18/32 [01:43<01:20,  5.74s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.74s/it, loss=0.222]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  59% 19/32 [01:49<01:14,  5.75s/it, loss=0.222]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  59% 19/32 [01:54<01:14,  5.75s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  62% 20/32 [01:54<01:08,  5.69s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  62% 20/32 [02:00<01:08,  5.69s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  66% 21/32 [02:00<01:02,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.71s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  69% 22/32 [02:06<00:57,  5.70s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  69% 22/32 [02:11<00:57,  5.70s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  72% 23/32 [02:11<00:51,  5.69s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  72% 23/32 [02:17<00:51,  5.69s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  75% 24/32 [02:17<00:45,  5.66s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 * Epoch (train):  75% 24/32 [02:23<00:45,  5.66s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  78% 25/32 [02:23<00:39,  5.69s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  78% 25/32 [02:29<00:39,  5.69s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  81% 26/32 [02:34<00:34,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  84% 27/32 [02:34<00:28,  5.72s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  84% 27/32 [02:40<00:28,  5.72s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  88% 28/32 [02:40<00:22,  5.69s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  88% 28/32 [02:46<00:22,  5.69s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  91% 29/32 [02:46<00:17,  5.68s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  91% 29/32 [02:51<00:17,  5.68s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  94% 30/32 [02:51<00:11,  5.66s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  94% 30/32 [02:57<00:11,  5.66s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  97% 31/32 [02:57<00:05,  5.71s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train):  97% 31/32 [03:02<00:05,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train): 100% 32/32 [03:02<00:00,  5.65s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (train): 100% 32/32 [03:02<00:00,  5.72s/it, loss=0.214]\n",
      "\n",
      "\n",
      "3/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.91s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.91s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.91s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.91s/it, loss=0.206]\n",
      "\n",
      "\n",
      "3/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.86s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.86s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.86s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "3/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.86s/it, loss=0.211]\n",
      "[2020-05-27 21:08:17,929] \n",
      "3/10 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): loss=0.2108\n",
      "3/10 * Epoch 3 (valid): loss=0.2092\n",
      "3/10 * Epoch 3 (test): loss=0.2141\n",
      "[2020-05-27 21:08:17,929] \n",
      "3/10 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): loss=0.2108\n",
      "3/10 * Epoch 3 (valid): loss=0.2092\n",
      "3/10 * Epoch 3 (test): loss=0.2141\n",
      "[2020-05-27 21:08:17,929] \n",
      "3/10 * Epoch 3 (_base): lr=0.0010 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): loss=0.2108\n",
      "3/10 * Epoch 3 (valid): loss=0.2092\n",
      "3/10 * Epoch 3 (test): loss=0.2141\n",
      "\n",
      "\n",
      "4/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   3% 1/32 [00:05<03:00,  5.83s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   3% 1/32 [00:11<03:00,  5.83s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   6% 2/32 [00:11<02:54,  5.82s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   6% 2/32 [00:17<02:54,  5.82s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   9% 3/32 [00:17<02:48,  5.82s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):   9% 3/32 [00:23<02:48,  5.82s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  12% 4/32 [00:23<02:42,  5.81s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  12% 4/32 [00:29<02:42,  5.81s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  16% 5/32 [00:29<02:36,  5.80s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  16% 5/32 [00:34<02:36,  5.80s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  22% 7/32 [00:40<02:25,  5.81s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.81s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  25% 8/32 [00:46<02:18,  5.79s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  25% 8/32 [00:52<02:18,  5.79s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  28% 9/32 [00:57<02:12,  5.74s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  31% 10/32 [00:57<02:06,  5.74s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  31% 10/32 [01:03<02:06,  5.74s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  34% 11/32 [01:03<02:00,  5.73s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  34% 11/32 [01:09<02:00,  5.73s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  38% 12/32 [01:09<01:54,  5.71s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  38% 12/32 [01:14<01:54,  5.71s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  41% 13/32 [01:14<01:48,  5.73s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  41% 13/32 [01:20<01:48,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  44% 14/32 [01:20<01:43,  5.72s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  47% 15/32 [01:26<01:36,  5.70s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  47% 15/32 [01:31<01:36,  5.70s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  50% 16/32 [01:31<01:30,  5.67s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  50% 16/32 [01:37<01:30,  5.67s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  53% 17/32 [01:37<01:25,  5.71s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  53% 17/32 [01:43<01:25,  5.71s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  56% 18/32 [01:43<01:20,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.72s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  59% 19/32 [01:49<01:14,  5.74s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  59% 19/32 [01:54<01:14,  5.74s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  62% 20/32 [01:54<01:08,  5.68s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  62% 20/32 [02:00<01:08,  5.68s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  66% 21/32 [02:00<01:02,  5.72s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.72s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  69% 22/32 [02:06<00:56,  5.70s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  69% 22/32 [02:11<00:56,  5.70s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  72% 23/32 [02:11<00:51,  5.68s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  72% 23/32 [02:17<00:51,  5.68s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  75% 24/32 [02:17<00:45,  5.67s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  75% 24/32 [02:23<00:45,  5.67s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  78% 25/32 [02:23<00:39,  5.70s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  78% 25/32 [02:29<00:39,  5.70s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  81% 26/32 [02:34<00:34,  5.74s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  84% 27/32 [02:34<00:28,  5.75s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  84% 27/32 [02:40<00:28,  5.75s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  88% 28/32 [02:40<00:22,  5.71s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  88% 28/32 [02:46<00:22,  5.71s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  91% 29/32 [02:46<00:17,  5.71s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  91% 29/32 [02:51<00:17,  5.71s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  94% 30/32 [02:51<00:11,  5.68s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  94% 30/32 [02:57<00:11,  5.68s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  97% 31/32 [02:57<00:05,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train):  97% 31/32 [03:03<00:05,  5.75s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.69s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.73s/it, loss=0.213]\n",
      "\n",
      "\n",
      "4/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.92s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.92s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.92s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.92s/it, loss=0.206]\n",
      "\n",
      "\n",
      "4/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.86s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.86s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.84s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "4/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.83s/it, loss=0.211]\n",
      "[2020-05-27 21:11:44,743] \n",
      "4/10 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): loss=0.2106\n",
      "4/10 * Epoch 4 (valid): loss=0.2096\n",
      "4/10 * Epoch 4 (test): loss=0.2145\n",
      "[2020-05-27 21:11:44,743] \n",
      "4/10 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): loss=0.2106\n",
      "4/10 * Epoch 4 (valid): loss=0.2096\n",
      "4/10 * Epoch 4 (test): loss=0.2145\n",
      "[2020-05-27 21:11:44,743] \n",
      "4/10 * Epoch 4 (_base): lr=0.0010 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): loss=0.2106\n",
      "4/10 * Epoch 4 (valid): loss=0.2096\n",
      "4/10 * Epoch 4 (test): loss=0.2145\n",
      "\n",
      "\n",
      "5/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   3% 1/32 [00:05<02:58,  5.76s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   3% 1/32 [00:11<02:58,  5.76s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   6% 2/32 [00:11<02:52,  5.76s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   6% 2/32 [00:17<02:52,  5.76s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   9% 3/32 [00:17<02:47,  5.77s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):   9% 3/32 [00:23<02:47,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  12% 4/32 [00:23<02:41,  5.76s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  12% 4/32 [00:28<02:41,  5.76s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  16% 5/32 [00:28<02:35,  5.75s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  16% 5/32 [00:34<02:35,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  19% 6/32 [00:34<02:29,  5.74s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  19% 6/32 [00:40<02:29,  5.74s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  22% 7/32 [00:40<02:23,  5.74s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  22% 7/32 [00:46<02:23,  5.74s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  25% 8/32 [00:46<02:18,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  25% 8/32 [00:51<02:18,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  28% 9/32 [00:51<02:11,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  28% 9/32 [00:57<02:11,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  31% 10/32 [00:57<02:06,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  31% 10/32 [01:03<02:06,  5.73s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  34% 11/32 [01:03<02:00,  5.73s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  34% 11/32 [01:08<02:00,  5.73s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  38% 12/32 [01:08<01:54,  5.71s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  38% 12/32 [01:14<01:54,  5.71s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  41% 13/32 [01:14<01:48,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  41% 13/32 [01:20<01:48,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  44% 14/32 [01:20<01:43,  5.73s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.71s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  47% 15/32 [01:31<01:37,  5.71s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  50% 16/32 [01:31<01:31,  5.71s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  50% 16/32 [01:37<01:31,  5.71s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  53% 17/32 [01:37<01:26,  5.76s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  53% 17/32 [01:43<01:26,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  56% 18/32 [01:43<01:20,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.77s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  59% 19/32 [01:49<01:14,  5.75s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  59% 19/32 [01:54<01:14,  5.75s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  62% 20/32 [01:54<01:08,  5.68s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  62% 20/32 [02:00<01:08,  5.68s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  66% 21/32 [02:00<01:02,  5.70s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.70s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  69% 22/32 [02:06<00:56,  5.70s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  69% 22/32 [02:11<00:56,  5.70s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  72% 23/32 [02:11<00:51,  5.69s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  72% 23/32 [02:17<00:51,  5.69s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  75% 24/32 [02:17<00:45,  5.67s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  75% 24/32 [02:23<00:45,  5.67s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  78% 25/32 [02:23<00:39,  5.70s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  78% 25/32 [02:28<00:39,  5.70s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  81% 26/32 [02:28<00:34,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  81% 26/32 [02:34<00:34,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  84% 27/32 [02:34<00:28,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  84% 27/32 [02:40<00:28,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  88% 28/32 [02:40<00:22,  5.70s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  88% 28/32 [02:45<00:22,  5.70s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  91% 29/32 [02:45<00:17,  5.69s/it, loss=0.197]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  91% 29/32 [02:51<00:17,  5.69s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  94% 30/32 [02:51<00:11,  5.67s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  94% 30/32 [02:57<00:11,  5.67s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  97% 31/32 [02:57<00:05,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train):  97% 31/32 [03:02<00:05,  5.72s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train): 100% 32/32 [03:02<00:00,  5.66s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (train): 100% 32/32 [03:02<00:00,  5.72s/it, loss=0.213]\n",
      "\n",
      "\n",
      "5/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.91s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.91s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.89s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.88s/it, loss=0.206]\n",
      "\n",
      "\n",
      "5/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.86s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.86s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.85s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "5/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.85s/it, loss=0.211]\n",
      "[2020-05-27 21:15:11,194] \n",
      "5/10 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): loss=0.2105\n",
      "5/10 * Epoch 5 (valid): loss=0.2096\n",
      "5/10 * Epoch 5 (test): loss=0.2145\n",
      "[2020-05-27 21:15:11,194] \n",
      "5/10 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): loss=0.2105\n",
      "5/10 * Epoch 5 (valid): loss=0.2096\n",
      "5/10 * Epoch 5 (test): loss=0.2145\n",
      "[2020-05-27 21:15:11,194] \n",
      "5/10 * Epoch 5 (_base): lr=0.0010 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): loss=0.2105\n",
      "5/10 * Epoch 5 (valid): loss=0.2096\n",
      "5/10 * Epoch 5 (test): loss=0.2145\n",
      "\n",
      "\n",
      "6/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   3% 1/32 [00:05<03:01,  5.84s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   3% 1/32 [00:11<03:01,  5.84s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   6% 2/32 [00:11<02:54,  5.83s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   6% 2/32 [00:17<02:54,  5.83s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   9% 3/32 [00:17<02:48,  5.81s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):   9% 3/32 [00:23<02:48,  5.81s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  12% 4/32 [00:23<02:42,  5.81s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  12% 4/32 [00:29<02:42,  5.81s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  16% 5/32 [00:29<02:36,  5.80s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 * Epoch (train):  16% 5/32 [00:34<02:36,  5.80s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.78s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.78s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  22% 7/32 [00:40<02:24,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  22% 7/32 [00:46<02:24,  5.79s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.80s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.78s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  28% 9/32 [00:57<02:12,  5.78s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  31% 10/32 [00:57<02:07,  5.79s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  31% 10/32 [01:03<02:07,  5.79s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  34% 11/32 [01:03<02:01,  5.79s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  34% 11/32 [01:09<02:01,  5.79s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  38% 12/32 [01:09<01:55,  5.79s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  38% 12/32 [01:15<01:55,  5.79s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  41% 13/32 [01:15<01:50,  5.80s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  41% 13/32 [01:21<01:50,  5.80s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  44% 14/32 [01:21<01:44,  5.80s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  44% 14/32 [01:26<01:44,  5.80s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  47% 15/32 [01:26<01:38,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  47% 15/32 [01:32<01:38,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  50% 16/32 [01:32<01:32,  5.76s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  50% 16/32 [01:38<01:32,  5.76s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.78s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  53% 17/32 [01:44<01:26,  5.78s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  56% 18/32 [01:44<01:20,  5.78s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.78s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  59% 19/32 [01:49<01:15,  5.78s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  59% 19/32 [01:55<01:15,  5.78s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.70s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.70s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  66% 21/32 [02:01<01:03,  5.73s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  66% 21/32 [02:07<01:03,  5.73s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  69% 22/32 [02:07<00:57,  5.74s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.74s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.72s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.72s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.70s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  75% 24/32 [02:24<00:45,  5.70s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  78% 25/32 [02:24<00:40,  5.72s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  78% 25/32 [02:29<00:40,  5.72s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.76s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.76s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.76s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.76s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  88% 28/32 [02:41<00:22,  5.72s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  88% 28/32 [02:47<00:22,  5.72s/it, loss=0.198]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  91% 29/32 [02:47<00:17,  5.72s/it, loss=0.198]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  91% 29/32 [02:52<00:17,  5.72s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  94% 30/32 [02:52<00:11,  5.70s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.70s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train):  97% 31/32 [03:04<00:05,  5.76s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.69s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.75s/it, loss=0.213]\n",
      "\n",
      "\n",
      "6/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.94s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.94s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\n",
      "\n",
      "\n",
      "6/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.94s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.94s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.91s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "6/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.90s/it, loss=0.211]\n",
      "[2020-05-27 21:18:39,047] \n",
      "6/10 * Epoch 6 (_base): lr=0.0010 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): loss=0.2104\n",
      "6/10 * Epoch 6 (valid): loss=0.2097\n",
      "6/10 * Epoch 6 (test): loss=0.2144\n",
      "[2020-05-27 21:18:39,047] \n",
      "6/10 * Epoch 6 (_base): lr=0.0010 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): loss=0.2104\n",
      "6/10 * Epoch 6 (valid): loss=0.2097\n",
      "6/10 * Epoch 6 (test): loss=0.2144\n",
      "[2020-05-27 21:18:39,047] \n",
      "6/10 * Epoch 6 (_base): lr=0.0010 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): loss=0.2104\n",
      "6/10 * Epoch 6 (valid): loss=0.2097\n",
      "6/10 * Epoch 6 (test): loss=0.2144\n",
      "\n",
      "\n",
      "7/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   3% 1/32 [00:05<03:00,  5.84s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   3% 1/32 [00:11<03:00,  5.84s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   6% 2/32 [00:11<02:55,  5.85s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   6% 2/32 [00:17<02:55,  5.85s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   9% 3/32 [00:17<02:49,  5.86s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):   9% 3/32 [00:23<02:49,  5.86s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  12% 4/32 [00:23<02:43,  5.85s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  12% 4/32 [00:29<02:43,  5.85s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  16% 5/32 [00:29<02:37,  5.85s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  16% 5/32 [00:35<02:37,  5.85s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  19% 6/32 [00:35<02:31,  5.84s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  19% 6/32 [00:40<02:31,  5.84s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  22% 7/32 [00:40<02:25,  5.82s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.82s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.81s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.81s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  28% 9/32 [00:58<02:12,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  31% 10/32 [00:58<02:07,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  31% 10/32 [01:03<02:07,  5.77s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  34% 11/32 [01:03<02:01,  5.76s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  34% 11/32 [01:09<02:01,  5.76s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  38% 12/32 [01:09<01:54,  5.74s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  38% 12/32 [01:15<01:54,  5.74s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  41% 13/32 [01:15<01:49,  5.77s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  41% 13/32 [01:21<01:49,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  44% 14/32 [01:21<01:43,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.74s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.72s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.72s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.75s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  53% 17/32 [01:44<01:26,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  56% 18/32 [01:44<01:20,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.77s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  59% 19/32 [01:49<01:15,  5.78s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  59% 19/32 [01:55<01:15,  5.78s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.70s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.70s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  66% 21/32 [02:01<01:03,  5.73s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  66% 21/32 [02:06<01:03,  5.73s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  69% 22/32 [02:06<00:57,  5.73s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.73s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.72s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.72s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.69s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  75% 24/32 [02:24<00:45,  5.69s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  78% 25/32 [02:24<00:40,  5.72s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  78% 25/32 [02:29<00:40,  5.72s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.77s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.77s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.77s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  88% 28/32 [02:41<00:22,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  88% 28/32 [02:47<00:22,  5.73s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  91% 29/32 [02:47<00:17,  5.73s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  91% 29/32 [02:52<00:17,  5.73s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  94% 30/32 [02:52<00:11,  5.70s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.70s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.76s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train):  97% 31/32 [03:04<00:05,  5.76s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.69s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.76s/it, loss=0.213]\n",
      "\n",
      "\n",
      "7/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.96s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.96s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.94s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\n",
      "\n",
      "\n",
      "7/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.90s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.90s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.89s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "7/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.89s/it, loss=0.211]\n",
      "[2020-05-27 21:22:06,910] \n",
      "7/10 * Epoch 7 (_base): lr=0.0010 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): loss=0.2104\n",
      "7/10 * Epoch 7 (valid): loss=0.2096\n",
      "7/10 * Epoch 7 (test): loss=0.2144\n",
      "[2020-05-27 21:22:06,910] \n",
      "7/10 * Epoch 7 (_base): lr=0.0010 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): loss=0.2104\n",
      "7/10 * Epoch 7 (valid): loss=0.2096\n",
      "7/10 * Epoch 7 (test): loss=0.2144\n",
      "[2020-05-27 21:22:06,910] \n",
      "7/10 * Epoch 7 (_base): lr=0.0010 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): loss=0.2104\n",
      "7/10 * Epoch 7 (valid): loss=0.2096\n",
      "7/10 * Epoch 7 (test): loss=0.2144\n",
      "\n",
      "\n",
      "8/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   3% 1/32 [00:05<02:59,  5.80s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   3% 1/32 [00:11<02:59,  5.80s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   6% 2/32 [00:11<02:53,  5.79s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   6% 2/32 [00:17<02:53,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   9% 3/32 [00:17<02:48,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):   9% 3/32 [00:23<02:48,  5.79s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  12% 4/32 [00:23<02:41,  5.78s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  12% 4/32 [00:28<02:41,  5.78s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  16% 5/32 [00:28<02:36,  5.81s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  16% 5/32 [00:34<02:36,  5.81s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  22% 7/32 [00:40<02:25,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.80s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  28% 9/32 [00:57<02:12,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  31% 10/32 [00:57<02:06,  5.77s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  31% 10/32 [01:03<02:06,  5.77s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  34% 11/32 [01:03<02:00,  5.76s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  34% 11/32 [01:09<02:00,  5.76s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  38% 12/32 [01:09<01:54,  5.75s/it, loss=0.205]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  38% 12/32 [01:15<01:54,  5.75s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  41% 13/32 [01:15<01:49,  5.77s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  41% 13/32 [01:20<01:49,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  44% 14/32 [01:20<01:43,  5.77s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  44% 14/32 [01:26<01:43,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.74s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.74s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.73s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.75s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  53% 17/32 [01:43<01:26,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  56% 18/32 [01:43<01:20,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  56% 18/32 [01:49<01:20,  5.76s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  59% 19/32 [01:49<01:14,  5.77s/it, loss=0.221]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  59% 19/32 [01:55<01:14,  5.77s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.69s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  62% 20/32 [02:00<01:08,  5.69s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  66% 21/32 [02:00<01:02,  5.71s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  66% 21/32 [02:06<01:02,  5.71s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  69% 22/32 [02:06<00:57,  5.72s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.72s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.72s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.72s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.71s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  75% 24/32 [02:23<00:45,  5.71s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  78% 25/32 [02:23<00:40,  5.74s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  78% 25/32 [02:29<00:40,  5.74s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  81% 26/32 [02:29<00:34,  5.78s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.78s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.77s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.77s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  88% 28/32 [02:41<00:22,  5.73s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/10 * Epoch (train):  88% 28/32 [02:46<00:22,  5.73s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  91% 29/32 [02:46<00:17,  5.72s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  91% 29/32 [02:52<00:17,  5.72s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  94% 30/32 [02:52<00:11,  5.70s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.70s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.75s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train):  97% 31/32 [03:03<00:05,  5.75s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.70s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (train): 100% 32/32 [03:03<00:00,  5.75s/it, loss=0.213]\n",
      "\n",
      "\n",
      "8/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.96s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.96s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.94s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\n",
      "\n",
      "\n",
      "8/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.90s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.90s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.88s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "8/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.87s/it, loss=0.211]\n",
      "[2020-05-27 21:25:34,496] \n",
      "8/10 * Epoch 8 (_base): lr=0.0010 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): loss=0.2102\n",
      "8/10 * Epoch 8 (valid): loss=0.2096\n",
      "8/10 * Epoch 8 (test): loss=0.2142\n",
      "[2020-05-27 21:25:34,496] \n",
      "8/10 * Epoch 8 (_base): lr=0.0010 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): loss=0.2102\n",
      "8/10 * Epoch 8 (valid): loss=0.2096\n",
      "8/10 * Epoch 8 (test): loss=0.2142\n",
      "[2020-05-27 21:25:34,496] \n",
      "8/10 * Epoch 8 (_base): lr=0.0010 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): loss=0.2102\n",
      "8/10 * Epoch 8 (valid): loss=0.2096\n",
      "8/10 * Epoch 8 (test): loss=0.2142\n",
      "\n",
      "\n",
      "9/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   3% 1/32 [00:05<03:00,  5.81s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   3% 1/32 [00:11<03:00,  5.81s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   6% 2/32 [00:11<02:54,  5.80s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   6% 2/32 [00:17<02:54,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   9% 3/32 [00:17<02:48,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):   9% 3/32 [00:23<02:48,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  12% 4/32 [00:23<02:42,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  12% 4/32 [00:28<02:42,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  16% 5/32 [00:28<02:36,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  16% 5/32 [00:34<02:36,  5.80s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  22% 7/32 [00:40<02:24,  5.80s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  22% 7/32 [00:46<02:24,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.80s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  28% 9/32 [00:52<02:13,  5.79s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  28% 9/32 [00:57<02:13,  5.79s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  31% 10/32 [00:57<02:07,  5.79s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  31% 10/32 [01:03<02:07,  5.79s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  34% 11/32 [01:03<02:01,  5.79s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  34% 11/32 [01:09<02:01,  5.79s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  38% 12/32 [01:09<01:55,  5.77s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  38% 12/32 [01:15<01:55,  5.77s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  41% 13/32 [01:15<01:50,  5.80s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  41% 13/32 [01:21<01:50,  5.80s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  44% 14/32 [01:21<01:44,  5.79s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  44% 14/32 [01:26<01:44,  5.79s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.76s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.73s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  53% 17/32 [01:44<01:26,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  56% 18/32 [01:44<01:21,  5.79s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  56% 18/32 [01:49<01:21,  5.79s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  59% 19/32 [01:49<01:15,  5.79s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  59% 19/32 [01:55<01:15,  5.79s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.72s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.72s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  66% 21/32 [02:01<01:03,  5.75s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  66% 21/32 [02:07<01:03,  5.75s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  69% 22/32 [02:07<00:57,  5.76s/it, loss=0.201]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.76s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.74s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.74s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.73s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  75% 24/32 [02:24<00:45,  5.73s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  78% 25/32 [02:24<00:40,  5.75s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  78% 25/32 [02:30<00:40,  5.75s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  81% 26/32 [02:30<00:34,  5.79s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  81% 26/32 [02:35<00:34,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  84% 27/32 [02:35<00:28,  5.79s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.79s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  88% 28/32 [02:41<00:23,  5.76s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  88% 28/32 [02:47<00:23,  5.76s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  91% 29/32 [02:47<00:17,  5.76s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  91% 29/32 [02:53<00:17,  5.76s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  94% 30/32 [02:53<00:11,  5.74s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  94% 30/32 [02:59<00:11,  5.74s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  97% 31/32 [02:59<00:05,  5.79s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train):  97% 31/32 [03:04<00:05,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.73s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.77s/it, loss=0.213]\n",
      "\n",
      "\n",
      "9/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.94s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.94s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.92s/it, loss=0.206]\n",
      "\n",
      "\n",
      "9/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.88s/it, loss=0.218]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.88s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.88s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "9/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.89s/it, loss=0.211]\n",
      "[2020-05-27 21:29:02,772] \n",
      "9/10 * Epoch 9 (_base): lr=0.0010 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): loss=0.2100\n",
      "9/10 * Epoch 9 (valid): loss=0.2097\n",
      "9/10 * Epoch 9 (test): loss=0.2143\n",
      "[2020-05-27 21:29:02,772] \n",
      "9/10 * Epoch 9 (_base): lr=0.0010 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): loss=0.2100\n",
      "9/10 * Epoch 9 (valid): loss=0.2097\n",
      "9/10 * Epoch 9 (test): loss=0.2143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-27 21:29:02,772] \n",
      "9/10 * Epoch 9 (_base): lr=0.0010 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): loss=0.2100\n",
      "9/10 * Epoch 9 (valid): loss=0.2097\n",
      "9/10 * Epoch 9 (test): loss=0.2143\n",
      "\n",
      "\n",
      "10/10 * Epoch (train):   0% 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   0% 0/32 [00:05<?, ?it/s, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   3% 1/32 [00:05<03:00,  5.81s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   3% 1/32 [00:11<03:00,  5.81s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   6% 2/32 [00:11<02:54,  5.83s/it, loss=0.216]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   6% 2/32 [00:17<02:54,  5.83s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   9% 3/32 [00:17<02:49,  5.83s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):   9% 3/32 [00:23<02:49,  5.83s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  12% 4/32 [00:23<02:42,  5.82s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  12% 4/32 [00:29<02:42,  5.82s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  16% 5/32 [00:29<02:36,  5.81s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  16% 5/32 [00:34<02:36,  5.81s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  19% 6/32 [00:34<02:30,  5.81s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  19% 6/32 [00:40<02:30,  5.81s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  22% 7/32 [00:40<02:25,  5.80s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  22% 7/32 [00:46<02:25,  5.80s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  25% 8/32 [00:46<02:19,  5.81s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  25% 8/32 [00:52<02:19,  5.81s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  28% 9/32 [00:52<02:12,  5.78s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  28% 9/32 [00:58<02:12,  5.78s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  31% 10/32 [00:58<02:07,  5.79s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  31% 10/32 [01:03<02:07,  5.79s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  34% 11/32 [01:03<02:01,  5.79s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  34% 11/32 [01:09<02:01,  5.79s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  38% 12/32 [01:09<01:55,  5.76s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  38% 12/32 [01:15<01:55,  5.76s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  41% 13/32 [01:15<01:50,  5.79s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  41% 13/32 [01:21<01:50,  5.79s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  44% 14/32 [01:21<01:44,  5.78s/it, loss=0.209]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  44% 14/32 [01:26<01:44,  5.78s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  47% 15/32 [01:26<01:37,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  47% 15/32 [01:32<01:37,  5.76s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  50% 16/32 [01:32<01:31,  5.75s/it, loss=0.207]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  50% 16/32 [01:38<01:31,  5.75s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  53% 17/32 [01:38<01:26,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  53% 17/32 [01:44<01:26,  5.77s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  56% 18/32 [01:44<01:21,  5.79s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  56% 18/32 [01:49<01:21,  5.79s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  59% 19/32 [01:49<01:15,  5.78s/it, loss=0.220]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  59% 19/32 [01:55<01:15,  5.78s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  62% 20/32 [01:55<01:08,  5.72s/it, loss=0.204]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  62% 20/32 [02:01<01:08,  5.72s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  66% 21/32 [02:01<01:03,  5.76s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  66% 21/32 [02:07<01:03,  5.76s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  69% 22/32 [02:07<00:57,  5.75s/it, loss=0.202]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  69% 22/32 [02:12<00:57,  5.75s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  72% 23/32 [02:12<00:51,  5.75s/it, loss=0.212]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  72% 23/32 [02:18<00:51,  5.75s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  75% 24/32 [02:18<00:45,  5.72s/it, loss=0.203]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  75% 24/32 [02:24<00:45,  5.72s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  78% 25/32 [02:24<00:40,  5.75s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  78% 25/32 [02:30<00:40,  5.75s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  81% 26/32 [02:30<00:34,  5.79s/it, loss=0.208]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  81% 26/32 [02:36<00:34,  5.79s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  84% 27/32 [02:36<00:28,  5.78s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  84% 27/32 [02:41<00:28,  5.78s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  88% 28/32 [02:41<00:22,  5.74s/it, loss=0.214]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  88% 28/32 [02:47<00:22,  5.74s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  91% 29/32 [02:47<00:17,  5.74s/it, loss=0.196]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  91% 29/32 [02:53<00:17,  5.74s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  94% 30/32 [02:53<00:11,  5.73s/it, loss=0.215]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  94% 30/32 [02:58<00:11,  5.73s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  97% 31/32 [02:58<00:05,  5.77s/it, loss=0.210]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train):  97% 31/32 [03:04<00:05,  5.77s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.71s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (train): 100% 32/32 [03:04<00:00,  5.77s/it, loss=0.213]\n",
      "\n",
      "\n",
      "10/10 * Epoch (valid):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (valid):   0% 0/2 [00:05<?, ?it/s, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (valid):  50% 1/2 [00:05<00:05,  5.94s/it, loss=0.213]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (valid):  50% 1/2 [00:11<00:05,  5.94s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.93s/it, loss=0.206]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (valid): 100% 2/2 [00:11<00:00,  5.92s/it, loss=0.206]\n",
      "\n",
      "\n",
      "10/10 * Epoch (test):   0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (test):   0% 0/2 [00:05<?, ?it/s, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (test):  50% 1/2 [00:05<00:05,  5.89s/it, loss=0.217]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (test):  50% 1/2 [00:11<00:05,  5.89s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.88s/it, loss=0.211]\u001b[A\u001b[A\n",
      "\n",
      "10/10 * Epoch (test): 100% 2/2 [00:11<00:00,  5.87s/it, loss=0.211]\n",
      "[2020-05-27 21:32:30,959] \n",
      "10/10 * Epoch 10 (_base): lr=0.0010 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): loss=0.2099\n",
      "10/10 * Epoch 10 (valid): loss=0.2098\n",
      "10/10 * Epoch 10 (test): loss=0.2140\n",
      "[2020-05-27 21:32:30,959] \n",
      "10/10 * Epoch 10 (_base): lr=0.0010 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): loss=0.2099\n",
      "10/10 * Epoch 10 (valid): loss=0.2098\n",
      "10/10 * Epoch 10 (test): loss=0.2140\n",
      "[2020-05-27 21:32:30,959] \n",
      "10/10 * Epoch 10 (_base): lr=0.0010 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): loss=0.2099\n",
      "10/10 * Epoch 10 (valid): loss=0.2098\n",
      "10/10 * Epoch 10 (test): loss=0.2140\n",
      "Top best models:\n",
      "logs/checkpoints/train.3.pth\t0.2092\n"
     ]
    }
   ],
   "source": [
    "import catalyst\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "loaders = {\n",
    "    'train': data.DataLoader(ExtDataset(ext_train_records, vocabulary, fse_model), batch_size=128, collate_fn=collate_fn),\n",
    "    'valid': data.DataLoader(ExtDataset(ext_val_records, vocabulary, fse_model), batch_size=128, collate_fn=collate_fn),\n",
    "    'test': data.DataLoader(ExtDataset(ext_test_records, vocabulary, fse_model), batch_size=128, collate_fn=collate_fn),\n",
    "}\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "runner = SupervisedRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir='./logs',\n",
    "    num_epochs=num_epochs,\n",
    "    criterion=criterion,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwqhK2dyKuGL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bd7f627b514d73aae32c2052d23e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/care1e55/.local/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count: 256\n",
      "Ref: атака американских военных на аэропорт багдада будет чревата тяжелыми последствиями, отметили в российском миде. прежде всего потому, что в результате этого погиб командующий силами специального назначения «аль-кудс» касем сулеймани. иранские власти предупредили сша, что «преступников ожидает суровая месть».\n",
      "Hyp: убийство командующего силами специального назначения «аль-кудс» корпуса стражей исламской революции (элитные части вооруженных сил ирана) генерала касема сулеймани может привести к тяжелым последствиям для мира в регионе, считают российские дипломаты. «данный шаг вашингтона чреват тяжелыми последствиями для регионального мира и стабильности. исходим из того, что подобные акции не способствуют нахождению развязок сложных проблем, накопившихся на ближнем востоке, а напротив, ведут к новому раунду эскалации напряженности в регионе», — говорится в опубликованном в пятницу комментарии мид россии о недавней атаке сша на аэропорт в багдаде, в результате которой погиб генерал. вместе с тем в дипведомстве отметили, что российская сторона восприняла сообщения о гибели сулеймани с тревогой. в комментарии риа «новости» представитель мид рф назвал этот шаг американских властей «авантюрным».\n",
      "BLEU:  0.383397885512414\n",
      "ROUGE:  {'rouge-1': {'f': 0.2529433998529658, 'p': 0.23209276172249507, 'r': 0.3059245092817107}, 'rouge-2': {'f': 0.10917240428214285, 'p': 0.09966983412779672, 'r': 0.13607381091784926}, 'rouge-l': {'f': 0.21057994506490585, 'p': 0.21052646318990106, 'r': 0.27752910138145953}}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "for i, item in tqdm(enumerate(data.DataLoader(ExtDataset(ext_test_records, vocabulary, fse_model), batch_size=1, collate_fn=collate_fn)), total=ext_test_records.shape[0]):\n",
    "    logits = model(item[\"features\"].to(device))[0] # Прямой проход\n",
    "    record = ext_test_records.iloc[i]\n",
    "    predicted_summary = []\n",
    "    for i, logit in enumerate(logits):\n",
    "        if logit > 0.05:\n",
    "            predicted_summary.append(record['sentences'][i])\n",
    "    if not predicted_summary:\n",
    "        predicted_summary.append(record['sentences'][torch.max(F.softmax(logits), dim=0)[1].item()])\n",
    "#         predicted_summary.append(record['sentences'][torch.max(logits, dim=0)[1].item()])\n",
    "    predicted_summary = \" \".join(predicted_summary)\n",
    "    references.append(record['summary'].lower())\n",
    "    predictions.append(predicted_summary)\n",
    "\n",
    "calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Kc0etEGfJ0p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]TextSummarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
